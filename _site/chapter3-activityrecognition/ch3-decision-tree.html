<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/mhealth-course/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/mhealth-course/assets/css/just-the-docs-head-nav.css"> <style id="jtd-nav-activation"> .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list-link, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list > .nav-list-item > .nav-list-link, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list > .nav-list-item > .nav-list > .nav-list-item:not(:nth-child(5)) > .nav-list-link { background-image: none; } .site-nav > .nav-list:not(:nth-child(1):not(.nav-category-list)) .nav-list-link, .site-nav .nav-list-link.external { background-image: none; } .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external):nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list > .nav-list-item:nth-child(5) > .nav-list-link { font-weight: 600; text-decoration: none; } .site-nav > .nav-category-list > .nav-list-item > .nav-list-expander svg, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list-expander svg, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list-expander svg { transform: rotate(-90deg); } .site-nav > .nav-category-list > .nav-list-item > .nav-list, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list { display: block; } </style> <script src="/mhealth-course/assets/js/vendor/lunr.min.js"></script> <script src="/mhealth-course/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/mhealth-course/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Decision Tree Classifier | mHealth Analytics</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Decision Tree Classifier" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Activity Recognition" /> <meta property="og:description" content="Activity Recognition" /> <link rel="canonical" href="http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html" /> <meta property="og:url" content="http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html" /> <meta property="og:site_name" content="mHealth Analytics" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Decision Tree Classifier" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Activity Recognition","headline":"Decision Tree Classifier","url":"http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" type="image/x-icon" href="/mhealth-course/assets/images/favicon.ico"> <!-- Mar 19, 2020: Just switched from cdn.mathjax.org to cdnjs based on https://www.mathjax.org/cdn-shutting-down/--> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "none" } }, tex2jax: { inlineMath: [['$','$'], ['\\(','\\)'], ['\[','\]']], displayMath: [['$$', '$$']], processEscapes: true, processEnvironments: true } }); MathJax.Hub.Register.StartupHook('TeX Jax Ready', function () { MathJax.InputJax.TeX.prefilterHooks.Add(function (data) { data.math = data.math.replace(/^% <!\[CDATA\[/, '').replace(/%\]\]>$/, ''); }); }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/mhealth-course/" class="site-title lh-tight"> mHealth Analytics </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Mobile Sensing &amp; Analytics category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/" class="nav-list-link">Mobile Sensing &amp; Analytics</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Data Smoothing category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter1-noise/chapter1.html" class="nav-list-link">Data Smoothing</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-intro.html" class="nav-list-link">Intro to Data Smoothing</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-timedomainfiltering.html" class="nav-list-link">Time-domain Smoothing</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-sampling-nyquist.html" class="nav-list-link">Sampling and Nyquist</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-freqdomainfiltering.html" class="nav-list-link">Freq-domain Filtering</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Step Counting category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter2-steps/chapter2.html" class="nav-list-link">Step Counting</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-intro.html" class="nav-list-link">Intro to Step Counting</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-stepcounter.html" class="nav-list-link">Step Detection Algorithm</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-calories.html" class="nav-list-link">Counting Calories</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Activity Recognition category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter3-activityrecognition/chapter3.html" class="nav-list-link">Activity Recognition</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html" class="nav-list-link">Detection vs Classification</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html" class="nav-list-link">Visualizing activities</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html" class="nav-list-link">Time domain features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html" class="nav-list-link">Freq domain features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html" class="nav-list-link">Decision Tree Classifier</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html" class="nav-list-link">Cardiac Risk Prediction</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html" class="nav-list-link">Overcoming Overfitting</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-classifier-performance.html" class="nav-list-link">Evaluating Classifier Performance</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Heart Rhythm Sensing category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter4-heartrhythm/chapter4.html" class="nav-list-link">Heart Rhythm Sensing</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter4-heartrhythm/ch4-ppg.html" class="nav-list-link">Photoplethysmography (PPG)</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter4-heartrhythm/ch4-ecg-ppg-analysis.html" class="nav-list-link">PPG Analysis</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Vocal Biomarkers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter5-voiceanalytics/chapter5.html" class="nav-list-link">Vocal Biomarkers</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter5-voiceanalytics/ch5-audiofeatures.html" class="nav-list-link">Audio Features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter5-voiceanalytics/ch5-audioclassification.html" class="nav-list-link">Audio Classification</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Measuring Sleep Patterns category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter6-sleepsensing/chapter6.html" class="nav-list-link">Measuring Sleep Patterns</a><ul class="nav-list"></ul></li></ul></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search mHealth Analytics" aria-label="Search mHealth Analytics" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/mhealth-course/">Mobile Sensing &amp; Analytics</a></li> <li class="breadcrumb-nav-list-item"><a href="/mhealth-course/chapter3-activityrecognition/chapter3.html">Activity Recognition</a></li> <li class="breadcrumb-nav-list-item"><span>Decision Tree Classifier</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h2 class="no_toc" id="human-activity-recognition"> <a href="#human-activity-recognition" class="anchor-heading" aria-labelledby="human-activity-recognition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Human Activity Recognition </h2> <h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of Contents </h2> <ol id="markdown-toc"> <li><a href="#decision-tree-classifier" id="markdown-toc-decision-tree-classifier">Decision tree classifier</a></li> <li><a href="#decision-tree-construction-using-entropy" id="markdown-toc-decision-tree-construction-using-entropy">Decision Tree Construction using Entropy</a> <ol> <li><a href="#entropy" id="markdown-toc-entropy">Entropy</a></li> </ol> </li> <li><a href="#multiclass-entropy" id="markdown-toc-multiclass-entropy">Multiclass Entropy</a> <ol> <li><a href="#information-gain" id="markdown-toc-information-gain">Information Gain</a></li> <li><a href="#building-the-tree-c45-algorithm" id="markdown-toc-building-the-tree-c45-algorithm">Building the Tree: C4.5 Algorithm</a></li> </ol> </li> <li><a href="#implementing-decision-trees-in-python" id="markdown-toc-implementing-decision-trees-in-python">Implementing Decision Trees in Python</a> <ol> <li><a href="#about-the-dataframe" id="markdown-toc-about-the-dataframe">About the DataFrame</a></li> <li><a href="#implementing-the-decision-tree-using-scikit-learn" id="markdown-toc-implementing-the-decision-tree-using-scikit-learn">Implementing the Decision Tree using Scikit-learn</a></li> <li><a href="#parameters-of-the-decisiontreeclassifier" id="markdown-toc-parameters-of-the-decisiontreeclassifier">Parameters of the DecisionTreeClassifier</a></li> </ol> </li> </ol><hr /> <h3 id="decision-tree-classifier"> <a href="#decision-tree-classifier" class="anchor-heading" aria-labelledby="decision-tree-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Decision tree classifier </h3> <p>Once we extract different features from the data, it’s time to build our classifier. At a high level, the goal of a classifier is to identify which of the above features that you obtained from your raw data is most useful in distinguishing between the different activities that you want to classify. There are many different classifiers, and an exhaustive summary would take too much time. But let me try to introduce one such classifier to provide an intuition for how such a method would work.</p> <p><strong>What is a decision tree?</strong> One of the most commonly used classifiers is a decision tree. The idea is simple and best explained with an example. Suppose you had the following six features - means along each of the x, y, z axes, and standard deviations along each of these axes. Given these features, you want to distinguish between standing, sitting, walking, running, biking, and driving. The key question is which of these features is most useful to distinguish between these activities.</p> <p><strong>How does a decision tree work?</strong> The idea behind a decision tree is to view this problem in a hierarchical manner. First, let us assume that we already constructed a decision tree (see figure above), and just try to understand how to use it, and what its telling us. In the above figure, the root node makes a decision on whether meanX &lt; 8.48 (this number is not important for now, focus on the concept). If true, then the decision is to take the right branch, and if false, take the left branch. Say we took the right branch - then we look at another decision, say checking if stddevX &lt; 11.36. Similarly, this decision tree proceeds by checking one feature after another until we finally get to the leaf node that tells us what our current state is. So, the process of using a decision tree to classify the current activity seems intuitive - its just a sequence of if-then-else statements with each statement checking the values of one or more parameters. But what does each of these branches really mean? What separates the left branch and the right branch?</p> <p><img src="images/image1.png" alt="drawing" width="500" /></p> <p><em>Figure 3: Example of a decision tree</em></p> <p><strong>Decision tree with an example.</strong> Let us look at an example of a decision tree and try to interpret its meaning. The root node separates {sitting, standing, driving} on the left branch from {biking, walking, running} on the right branch. Clearly, this node is identifying some feature that separates between more sedentary activities from the more active ones. This makes sense - if we had to think of an algorithm, we would perhaps do the same thing. Let’s now look at the decision nodes at the next level down starting with the left branch. This node separates driving from other sedentary activities (sitting, standing) by looking at the standard deviation of the Z axis. This seems to make sense as well - driving will cause lots of vibrations due to the car and the road, whereas we are unlikely to see these vibrations when you are sitting on a chair or standing (unless there’s an earthquake!). The corresponding decision node on the right branch seems to be doing something similar - it uses standard deviation on the vertical axis to distinguish between biking, and other physical activities such as running and walking. The intuition is that biking on the road is likely to have small vibrations because of the road whereas running or walking has large variations due to acceleration changes for each step (as you observed in the pedometer case study). The remaining decision nodes in the left branch no longer use the standard deviation, and only use the mean to separate sitting and standing. This is easiest to understand from Figure 1 where we saw the raw accelerometer signal for sitting and standing. Both are flat, so there’s little to be gained from looking at standard deviation, but they have different averages, so this is the most useful feature for separating the classes. On the right branch, walking vs running uses the difference in standard deviations between these two - intuition would suggest that running has more vibrations than walking. In summary, a decision tree works by identifying which features best separate the categories, and builds it as a tree structure.</p> <p>Perhaps a simpler approach to understand how a decision tree works is to view it as a series of questions. Consider a game where your friend has chosen an activity class, and you need to guess it. You are allowed a sequence of questions, each of which could be about some characteristic about the classes. What is the minimal number of questions you might ask? In this example, the first question you might as is: Is the user in a sedentary or active state? If the answer is “active”, you might ask further questions to refine the state. The decision tree can be viewed as learning the best sequence of questions given the data and its characteristics.</p> <h3 id="decision-tree-construction-using-entropy"> <a href="#decision-tree-construction-using-entropy" class="anchor-heading" aria-labelledby="decision-tree-construction-using-entropy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Decision Tree Construction using Entropy </h3> <p>We now understand how a decision tree works but how do we build the decision tree in the first place. The primary challenge in the construction of a decision tree is deciding on which attributes to split and in what order. One of the popular algorithms to handle this is C4.5, which uses entropy as a metric to determine the best split.</p> <h4 id="entropy"> <a href="#entropy" class="anchor-heading" aria-labelledby="entropy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Entropy </h4> <p>Entropy, derived from information theory, measures the level of uncertainty or randomness. In the context of decision trees, it quantifies the randomness or impurity in a label set. The for entropy for a binary classification (for simplicity) is given as:</p> <p>$ \text{Entropy}(S) = -p_+ \log_2(p_+) - p_- \log_2(p_-) $</p> <p>Where:</p> <ul> <li>$ p_+ $ is the proportion of positive examples in $ S $</li> <li>$ p_- $ is the proportion of negative examples in $ S $</li> </ul> <p>If the sample is completely homogeneous (either entirely positive or entirely negative), the entropy is 0. If the sample is an equally divided mixture of positive and negative examples, the entropy is 1 (maximum).</p> <p>Absolutely! The concept of entropy can be extended naturally to multiclass classification scenarios.</p> <h3 id="multiclass-entropy"> <a href="#multiclass-entropy" class="anchor-heading" aria-labelledby="multiclass-entropy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multiclass Entropy </h3> <p>For binary classification, the entropy formula looks at the proportions of positive and negative examples. When you have multiple classes, the formula is simply expanded to account for all these classes.</p> <p>Given $ C $ as the set of all classes, for a multiclass problem, entropy $ S $ is defined as:</p> <p>$ \text{Entropy}(S) = - \sum_{c \in C} p_c \log_2(p_c) $</p> <p>Where:</p> <ul> <li>$ p_c $ is the proportion of samples belonging to class $ c $ in $ S $.</li> </ul> <p>For a sample that is completely homogeneous with respect to a class (i.e., all members of the sample belong to that class), the entropy is 0, just like the binary case. The maximum value of entropy depends on the number of classes. For instance, if there are 3 classes, and each class is equally represented in the sample, the entropy is $- \frac{1}{3} \log_2(\frac{1}{3}) \times 3 = \log_2(3) \approx 1.58$. As you can see, the more classes, the higher the maximum entropy.</p> <h4 id="information-gain"> <a href="#information-gain" class="anchor-heading" aria-labelledby="information-gain"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Information Gain </h4> <p>To determine which attribute should be selected as the decision node, we use a metric called Information Gain. It calculates the effectiveness of an attribute in classifying the training data. The attribute with the highest information gain is chosen as the decision node at a particular level.</p> <p>The formula to compute Information Gain is:</p> <p>$\text{Information Gain}(S, A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v) $</p> <p>Where:</p> <ul> <li>$ S $ is the set of examples</li> <li>$ A $ is an attribute</li> <li>$ S_v $ is the subset of $ S $ for which attribute $ A $ has value $ v $</li> </ul> <h4 id="building-the-tree-c45-algorithm"> <a href="#building-the-tree-c45-algorithm" class="anchor-heading" aria-labelledby="building-the-tree-c45-algorithm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Building the Tree: C4.5 Algorithm </h4> <p><strong>Pseudocode:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>function BuildTree(samples, attributes)
    if all samples have the same label
        return a leaf node with that label
    if attributes is empty
        return a leaf node with the most common label of samples
    select the best attribute A using Information Gain
    create a tree T with A as the root
    for each value v of A
        add a branch to T for the test A = v
        let Sv be the subset of samples where A = v
        if Sv is empty
            add a leaf node with the most common label of samples
        else
            add BuildTree(Sv, attributes - {A}) to T
    return T
</code></pre></div></div> <p>To summarize:</p> <ol> <li>For each attribute, calculate the entropy before the split and the weighted entropy of each possible split, then compute the information gain.</li> <li>Choose the attribute with the highest information gain for the split, creating a node.</li> <li>Divide the dataset by the values of the chosen attribute, then recursively build subtrees in the same manner.</li> <li>The recursion stops once all data belongs to a single class or no attributes remain.</li> </ol> <p>This process allows for a decision tree that maximizes the reduction in randomness or impurity at each level, making it efficient in making predictions on unseen data.</p> <p>Note: While understanding the construction is crucial, in practical scenarios, libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code> offer optimized implementations of the Decision Trees that abstract away these internal details, making it easier to apply to real-world problems.</p> <h3 id="implementing-decision-trees-in-python"> <a href="#implementing-decision-trees-in-python" class="anchor-heading" aria-labelledby="implementing-decision-trees-in-python"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing Decision Trees in Python </h3> <p>Once we have our feature vectors prepared from raw data, we can use them to train a decision tree classifier. Decision trees are a popular algorithm because of their interpretability and ease of use.</p> <h4 id="about-the-dataframe"> <a href="#about-the-dataframe" class="anchor-heading" aria-labelledby="about-the-dataframe"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> About the DataFrame </h4> <p>Let us assume we have a pandas DataFrame <code class="language-plaintext highlighter-rouge">resampled_data</code> with the following columns: <code class="language-plaintext highlighter-rouge">x_mean</code>, <code class="language-plaintext highlighter-rouge">y_mean</code>, <code class="language-plaintext highlighter-rouge">z_mean</code>, <code class="language-plaintext highlighter-rouge">x_std</code>, <code class="language-plaintext highlighter-rouge">y_std</code>, <code class="language-plaintext highlighter-rouge">z_std</code> (and potentially other feature columns) and a <code class="language-plaintext highlighter-rouge">label</code> column which refers to the activities you are attempting to classify. Each row in this DataFrame corresponds to a resampled window of data where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">x_mean</code>, <code class="language-plaintext highlighter-rouge">y_mean</code>, <code class="language-plaintext highlighter-rouge">z_mean</code>: Mean values of the X, Y, Z accelerometer readings, respectively.</li> <li><code class="language-plaintext highlighter-rouge">x_std</code>, <code class="language-plaintext highlighter-rouge">y_std</code>, <code class="language-plaintext highlighter-rouge">z_std</code>: Standard deviations of the X, Y, Z accelerometer readings, respectively.</li> <li><code class="language-plaintext highlighter-rouge">label</code>: The label or class of that particular window (e.g., ‘Sitting’, ‘Walking’).</li> </ul> <h4 id="implementing-the-decision-tree-using-scikit-learn"> <a href="#implementing-the-decision-tree-using-scikit-learn" class="anchor-heading" aria-labelledby="implementing-the-decision-tree-using-scikit-learn"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing the Decision Tree using Scikit-learn </h4> <p>Scikit-learn is a popular Python library for implementing machine learning algorithms. Here’s how you can use it to implement a decision tree:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Split the data into train and test sets
</span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'x_mean'</span><span class="p">,</span> <span class="s">'y_mean'</span><span class="p">,</span> <span class="s">'z_mean'</span><span class="p">,</span> <span class="s">'x_std'</span><span class="p">,</span> <span class="s">'y_std'</span><span class="p">,</span> <span class="s">'z_std'</span><span class="p">]</span> <span class="c1"># Add other feature columns if needed
</span><span class="n">X</span> <span class="o">=</span> <span class="n">resampled_data</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">resampled_data</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create the decision tree classifier and train it
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s">'entropy'</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># Using Gini impurity as the criterion
</span><span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div> <h4 id="parameters-of-the-decisiontreeclassifier"> <a href="#parameters-of-the-decisiontreeclassifier" class="anchor-heading" aria-labelledby="parameters-of-the-decisiontreeclassifier"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Parameters of the DecisionTreeClassifier </h4> <ul> <li><code class="language-plaintext highlighter-rouge">criterion</code>: The function to measure the quality of a split. We are using ‘entropy’ for the information gain.</li> <li><code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of the tree. If <code class="language-plaintext highlighter-rouge">None</code>, then nodes are expanded until all leaves are pure or until all leaves contain less than <code class="language-plaintext highlighter-rouge">min_samples_split</code> samples.</li> <li><code class="language-plaintext highlighter-rouge">random_state</code>: Seed used by the random number generator for reproducibility.</li> </ul> <p>There are several other parameters you can set for the DecisionTreeClassifier, such as <code class="language-plaintext highlighter-rouge">min_samples_split</code>, <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>, etc., which can help in regularizing the tree, preventing overfitting, or otherwise influencing the way the tree grows.</p> <p>After training, you can evaluate the classifier using metrics like accuracy, precision, recall, and F1-score using the true labels (<code class="language-plaintext highlighter-rouge">y_test</code>) and the predicted labels (<code class="language-plaintext highlighter-rouge">y_pred</code>).</p> <p>Remember to also consider regularizing your decision tree (using parameters like <code class="language-plaintext highlighter-rouge">max_depth</code> or <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>) to prevent it from overfitting, especially if you have a large number of features or a small amount of training data.</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
