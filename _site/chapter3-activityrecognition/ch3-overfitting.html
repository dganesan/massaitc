<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/mhealth-course/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/mhealth-course/assets/css/just-the-docs-head-nav.css"> <style id="jtd-nav-activation"> .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list-link, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list > .nav-list-item > .nav-list-link, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external) > .nav-list > .nav-list-item > .nav-list > .nav-list-item:not(:nth-child(7)) > .nav-list-link { background-image: none; } .site-nav > .nav-list:not(:nth-child(1):not(.nav-category-list)) .nav-list-link, .site-nav .nav-list-link.external { background-image: none; } .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:not(.external):nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list > .nav-list-item:nth-child(7) > .nav-list-link { font-weight: 600; text-decoration: none; } .site-nav > .nav-category-list > .nav-list-item > .nav-list-expander svg, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list-expander svg, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list-expander svg { transform: rotate(-90deg); } .site-nav > .nav-category-list > .nav-list-item > .nav-list, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list, .site-nav > .nav-list:nth-child(1):not(.nav-category-list) > .nav-list-item:nth-child(1) > .nav-list > .nav-list-item:nth-child(3) > .nav-list { display: block; } </style> <script src="/mhealth-course/assets/js/vendor/lunr.min.js"></script> <script src="/mhealth-course/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/mhealth-course/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Overcoming Overfitting | mHealth Analytics</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Overcoming Overfitting" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Activity Recognition" /> <meta property="og:description" content="Activity Recognition" /> <link rel="canonical" href="http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html" /> <meta property="og:url" content="http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html" /> <meta property="og:site_name" content="mHealth Analytics" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Overcoming Overfitting" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Activity Recognition","headline":"Overcoming Overfitting","url":"http://localhost:4000/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" type="image/x-icon" href="/mhealth-course/assets/images/favicon.ico"> <!-- Mar 19, 2020: Just switched from cdn.mathjax.org to cdnjs based on https://www.mathjax.org/cdn-shutting-down/--> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "none" } }, tex2jax: { inlineMath: [['$','$'], ['\\(','\\)'], ['\[','\]']], displayMath: [['$$', '$$']], processEscapes: true, processEnvironments: true } }); MathJax.Hub.Register.StartupHook('TeX Jax Ready', function () { MathJax.InputJax.TeX.prefilterHooks.Add(function (data) { data.math = data.math.replace(/^% <!\[CDATA\[/, '').replace(/%\]\]>$/, ''); }); }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/mhealth-course/" class="site-title lh-tight"> mHealth Analytics </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Mobile Sensing &amp; Analytics category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/" class="nav-list-link">Mobile Sensing &amp; Analytics</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Data Smoothing category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter1-noise/chapter1.html" class="nav-list-link">Data Smoothing</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-intro.html" class="nav-list-link">Intro to Data Smoothing</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-timedomainfiltering.html" class="nav-list-link">Time-domain Smoothing</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-sampling-nyquist.html" class="nav-list-link">Sampling and Nyquist</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter1-noise/ch1-freqdomainfiltering.html" class="nav-list-link">Freq-domain Filtering</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Step Counting category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter2-steps/chapter2.html" class="nav-list-link">Step Counting</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-intro.html" class="nav-list-link">Intro to Step Counting</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-stepcounter.html" class="nav-list-link">Step Detection Algorithm</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter2-steps/ch2-calories.html" class="nav-list-link">Counting Calories</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Activity Recognition category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter3-activityrecognition/chapter3.html" class="nav-list-link">Activity Recognition</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html" class="nav-list-link">Detection vs Classification</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html" class="nav-list-link">Visualizing activities</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html" class="nav-list-link">Time domain features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html" class="nav-list-link">Freq domain features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html" class="nav-list-link">Decision Tree Classifier</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html" class="nav-list-link">Cardiac Risk Prediction</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html" class="nav-list-link">Overcoming Overfitting</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter3-activityrecognition/ch3-classifier-performance.html" class="nav-list-link">Evaluating Classifier Performance</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Heart Rhythm Sensing category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter4-heartrhythm/chapter4.html" class="nav-list-link">Heart Rhythm Sensing</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter4-heartrhythm/ch4-ppg.html" class="nav-list-link">Photoplethysmography (PPG)</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter4-heartrhythm/ch4-ecg-ppg-analysis.html" class="nav-list-link">PPG Analysis</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Vocal Biomarkers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter5-voiceanalytics/chapter5.html" class="nav-list-link">Vocal Biomarkers</a><ul class="nav-list"><li class="nav-list-item"> <a href="/mhealth-course/chapter5-voiceanalytics/ch5-audiofeatures.html" class="nav-list-link">Audio Features</a> </li><li class="nav-list-item"> <a href="/mhealth-course/chapter5-voiceanalytics/ch5-audioclassification.html" class="nav-list-link">Audio Classification</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Measuring Sleep Patterns category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mhealth-course/chapter6-sleepsensing/chapter6.html" class="nav-list-link">Measuring Sleep Patterns</a><ul class="nav-list"></ul></li></ul></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search mHealth Analytics" aria-label="Search mHealth Analytics" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/mhealth-course/">Mobile Sensing &amp; Analytics</a></li> <li class="breadcrumb-nav-list-item"><a href="/mhealth-course/chapter3-activityrecognition/chapter3.html">Activity Recognition</a></li> <li class="breadcrumb-nav-list-item"><span>Overcoming Overfitting</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h2 class="no_toc" id="human-activity-recognition"> <a href="#human-activity-recognition" class="anchor-heading" aria-labelledby="human-activity-recognition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Human Activity Recognition </h2> <h2 class="no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of Contents </h2> <ol id="markdown-toc"> <li><a href="#introduction-to-overfitting" id="markdown-toc-introduction-to-overfitting">Introduction to Overfitting</a> <ol> <li><a href="#why-does-overfitting-occur" id="markdown-toc-why-does-overfitting-occur">Why Does Overfitting Occur?</a></li> <li><a href="#a-closer-look-with-cardiac-risk-assessment" id="markdown-toc-a-closer-look-with-cardiac-risk-assessment">A Closer Look with Cardiac Risk Assessment</a></li> </ol> </li> <li><a href="#addressing-overfitting-basic-approaches" id="markdown-toc-addressing-overfitting-basic-approaches">Addressing Overfitting: Basic Approaches</a> <ol> <li><a href="#train-test-split" id="markdown-toc-train-test-split">Train-Test Split</a></li> </ol> </li> <li><a href="#the-role-of-validation-sets" id="markdown-toc-the-role-of-validation-sets">The Role of Validation Sets</a></li> <li><a href="#k-fold-cross-validation-beyond-simple-splits" id="markdown-toc-k-fold-cross-validation-beyond-simple-splits">K-fold Cross-Validation: Beyond Simple Splits</a> <ol> <li><a href="#understanding-k-fold-cross-validation" id="markdown-toc-understanding-k-fold-cross-validation">Understanding K-fold Cross-Validation</a></li> <li><a href="#advantages-of-k-fold-cross-validation" id="markdown-toc-advantages-of-k-fold-cross-validation">Advantages of K-fold Cross-Validation</a></li> <li><a href="#calculating-error-in-k-fold-cross-validation" id="markdown-toc-calculating-error-in-k-fold-cross-validation">Calculating Error in K-fold Cross-Validation</a></li> <li><a href="#implementing-train-test-splitting--in-python" id="markdown-toc-implementing-train-test-splitting--in-python">Implementing Train-Test Splitting in Python</a> <ol> <li><a href="#optimizing-tree-depth-to-prevent-overfitting" id="markdown-toc-optimizing-tree-depth-to-prevent-overfitting">Optimizing Tree Depth to Prevent Overfitting</a></li> <li><a href="#other-strategies" id="markdown-toc-other-strategies">Other Strategies:</a></li> </ol> </li> <li><a href="#implementing-k-fold-cross-validation-in-python" id="markdown-toc-implementing-k-fold-cross-validation-in-python">Implementing k-fold Cross-Validation in Python</a> <ol> <li><a href="#combining-cross-validation-with-hyperparameter-tuning" id="markdown-toc-combining-cross-validation-with-hyperparameter-tuning">Combining Cross-Validation with Hyperparameter Tuning</a></li> </ol> </li> </ol> </li> </ol><hr /> <h2 id="introduction-to-overfitting"> <a href="#introduction-to-overfitting" class="anchor-heading" aria-labelledby="introduction-to-overfitting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction to Overfitting </h2> <p>Overfitting is a prevalent phenomenon in machine learning where a model learns to fit the training data too closely, capturing even its noise and outliers. This means that while it performs exceptionally well on training data, its performance degrades on unseen data.</p> <h3 id="why-does-overfitting-occur"> <a href="#why-does-overfitting-occur" class="anchor-heading" aria-labelledby="why-does-overfitting-occur"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Does Overfitting Occur? </h3> <p>In the context of decision trees, the depth or complexity of the tree plays a significant role in overfitting. A decision tree that is too deep will have leaves with very specific decisions, often based on individual or minimal data points. Such granularity can mean the tree is fitting to the noise in the data rather than the underlying patterns. While overfitting is particularly pronounced in decision trees due to their hierarchical nature, it’s essential to note that overfitting can occur in any machine learning algorithm.</p> <p>Noise in the data further exacerbates overfitting. In real-world datasets, it’s not uncommon to find inconsistencies or errors—these are referred to as noise. When a model, like a deep decision tree, learns from such data, it might treat this noise as valid patterns, leading to overfitting.</p> <h3 id="a-closer-look-with-cardiac-risk-assessment"> <a href="#a-closer-look-with-cardiac-risk-assessment" class="anchor-heading" aria-labelledby="a-closer-look-with-cardiac-risk-assessment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Closer Look with Cardiac Risk Assessment </h3> <p>Imagine our cardiac risk assessment dataset has some inconsistencies. Maybe a few healthy individuals have been mistakenly labeled as high risk due to errors in manual data entry. A deep decision tree might create specific branches to classify these individuals correctly based on the noisy data. However, when we introduce new data, these branches misclassify because they were based on incorrect data in the first place.</p> <h2 id="addressing-overfitting-basic-approaches"> <a href="#addressing-overfitting-basic-approaches" class="anchor-heading" aria-labelledby="addressing-overfitting-basic-approaches"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Addressing Overfitting: Basic Approaches </h2> <p>Cross validation is an evaluation method to identify how well the classifier will perform for data it has not already seen. One way is to not use the entire data set when training a learner. Some of the data is removed before training begins. Then when training is done, the data that was removed can be used to test the performance of the learned model on ``new’’ data. This is the basic idea for a whole class of model evaluation methods called <em>cross validation</em>.</p> <h3 id="train-test-split"> <a href="#train-test-split" class="anchor-heading" aria-labelledby="train-test-split"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Train-Test Split </h3> <p>A foundational approach to handle overfitting by cross-validation is dividing the data into a training set and a test set. This is also referred to as the <strong>holdout method</strong> and is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. A common rule of thumb is to use 70\% of the dataset for training and 30\% for testing. Dividing the data into training and test subsets is usually done randomly, in order to guarantee that there is no systematic error in the process.</p> <ul> <li><strong>Train Model</strong>: Use the training set to teach the model.</li> <li><strong>Evaluate</strong>: Test its performance on the unseen test set.</li> <li><strong>Tweak and Repeat</strong>: Based on the test set performance, make changes to the model, train it again, and keep repeating this loop.</li> </ul> <p>By evaluating the model on the test set, which it hasn’t seen during training, we get a clearer picture of how the model performs on new, unseen data. The idea is to select the model version that gives the best performance on the test set. However, its evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made.</p> <h2 id="the-role-of-validation-sets"> <a href="#the-role-of-validation-sets" class="anchor-heading" aria-labelledby="the-role-of-validation-sets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Role of Validation Sets </h2> <p>While the train-test methodology helps, there’s still room for over-optimizing on the test set, especially when we make numerous tweaks based on test set performance. To avoid this, we introduce a third dataset called the validation set.</p> <ul> <li>Train the model on the training set.</li> <li>Tweak and tune the model based on performance on the validation set.</li> <li>Only once you’ve finalized the model, evaluate it on the test set to get an unbiased performance metric.</li> </ul> <p>Having a separate validation set allows us to make adjustments without biasing our model to the test data.</p> <h2 id="k-fold-cross-validation-beyond-simple-splits"> <a href="#k-fold-cross-validation-beyond-simple-splits" class="anchor-heading" aria-labelledby="k-fold-cross-validation-beyond-simple-splits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> K-fold Cross-Validation: Beyond Simple Splits </h2> <p>While train-test and train-validation-test splits are fundamental to avoiding overfitting, they might not always be enough. This is where k-fold cross-validation comes into play.</p> <h3 id="understanding-k-fold-cross-validation"> <a href="#understanding-k-fold-cross-validation" class="anchor-heading" aria-labelledby="understanding-k-fold-cross-validation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding K-fold Cross-Validation </h3> <p>Instead of dividing the dataset into two or three static sets, k-fold cross-validation divides the dataset into ( k ) different subsets (or “folds”). The model is trained and evaluated ( k ) times, each time using a different fold as the test set and the remaining ( k-1 ) folds combined as the training set. This process is iterated until every fold has been used as the test set.</p> <p>For example, in a 5-fold cross-validation:</p> <ol> <li>In the first iteration, Fold 1 is the test set, and Folds 2-5 combined are the training set.</li> <li>In the second iteration, Fold 2 becomes the test set, while Folds 1, 3, 4, and 5 are the training set. … and so on until the fifth iteration.</li> </ol> <h3 id="advantages-of-k-fold-cross-validation"> <a href="#advantages-of-k-fold-cross-validation" class="anchor-heading" aria-labelledby="advantages-of-k-fold-cross-validation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Advantages of K-fold Cross-Validation </h3> <ol> <li> <p><strong>Better Utilization of Data</strong>: Especially in scenarios where the dataset is limited in size, k-fold cross-validation ensures that every data point has been in the test set exactly once and in the training set ( k-1 ) times. This comprehensive utilization can lead to a more reliable estimate of model performance.</p> </li> <li> <p><strong>Reduced Variability</strong>: By averaging the results from the ( k ) folds, we can reduce the variance associated with a single random train-test split. This leads to a more stable and generalized model assessment.</p> </li> <li> <p><strong>Mitigates Overfitting</strong>: With multiple train-test splits, the model’s ability to generalize is tested more rigorously, making it less likely that the model is overfitting to a specific subset of the data.</p> </li> </ol> <h3 id="calculating-error-in-k-fold-cross-validation"> <a href="#calculating-error-in-k-fold-cross-validation" class="anchor-heading" aria-labelledby="calculating-error-in-k-fold-cross-validation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Calculating Error in K-fold Cross-Validation </h3> <p>The final performance metric in k-fold cross-validation is typically the average of the errors across the ( k ) iterations. If ( E_i ) represents the error in the ( i^{th} ) fold, the overall error ( E ) is calculated as:</p> <p>[ E = \frac{1}{k} \sum_{i=1}^{k} E_i ]</p> <p>By taking the average error across the folds, we ensure that the model’s performance is not overly influenced by any single partition of the data.</p> <p>Overfitting remains one of the primary challenges in machine learning. Recognizing its signs and employing techniques like train-test splits and validation sets are crucial first steps in combating it. K-fold cross-validation, with its systematic approach to training and evaluating, offers a robust mechanism to understand a model’s performance. While it can be computationally more intensive, the insights gained from multiple evaluations often outweigh the costs, leading to more trustworthy and generalizable models.</p> <h3 id="implementing-train-test-splitting--in-python"> <a href="#implementing-train-test-splitting--in-python" class="anchor-heading" aria-labelledby="implementing-train-test-splitting--in-python"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing Train-Test Splitting in Python </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'x_mean'</span><span class="p">,</span> <span class="s">'y_mean'</span><span class="p">,</span> <span class="s">'z_mean'</span><span class="p">,</span> <span class="s">'x_std'</span><span class="p">,</span> <span class="s">'y_std'</span><span class="p">,</span> <span class="s">'z_std'</span><span class="p">]</span>  <span class="c1"># and other feature columns if needed
</span><span class="n">X</span> <span class="o">=</span> <span class="n">resampled_data</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">resampled_data</span><span class="p">[</span><span class="s">'label'</span><span class="p">]</span>

<span class="c1"># Split the data
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">test_size</code> parameter defines the proportion of the dataset to include in the test split. A common split ratio is 80\% training and 20\% testing.</p> <h4 id="optimizing-tree-depth-to-prevent-overfitting"> <a href="#optimizing-tree-depth-to-prevent-overfitting" class="anchor-heading" aria-labelledby="optimizing-tree-depth-to-prevent-overfitting"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimizing Tree Depth to Prevent Overfitting </h4> <p>A simple way to prevent overfitting in decision trees is to control the depth of the tree. A tree that is too deep might overfit the training data, while a very shallow tree might underfit.</p> <p>To find the optimal depth, you can:</p> <ol> <li>Train multiple trees with varying depths.</li> <li>Evaluate each tree on the test set.</li> <li>Choose the depth that gives the best performance on the test set.</li> </ol> <p>Here’s a simple example using scikit-learn:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">best_depth</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">best_accuracy</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Check for depths from 1 to 10
</span><span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">acc</span> <span class="o">&gt;</span> <span class="n">best_accuracy</span><span class="p">:</span>
        <span class="n">best_accuracy</span> <span class="o">=</span> <span class="n">acc</span>
        <span class="n">best_depth</span> <span class="o">=</span> <span class="n">depth</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Best depth is: </span><span class="si">{</span><span class="n">best_depth</span><span class="si">}</span><span class="s"> with accuracy: </span><span class="si">{</span><span class="n">best_accuracy</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="other-strategies"> <a href="#other-strategies" class="anchor-heading" aria-labelledby="other-strategies"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Other Strategies: </h4> <ol> <li><strong>Pruning</strong>: Instead of letting the tree grow to its maximum depth, you can prune it to remove nodes that add little power to the classification.</li> <li><strong>Minimum Split Size</strong>: Set the minimum number of samples required to split an internal node.</li> <li><strong>Minimum Leaf Size</strong>: Set the minimum number of samples required to be at a leaf node.</li> <li><strong>Cross-Validation</strong>: Instead of a single train/test split, use k-fold cross-validation to get an average performance metric, which will provide a more robust performance estimate.</li> <li><strong>Feature Importance</strong>: Some features might be noisy. Decision trees in scikit-learn provide a <code class="language-plaintext highlighter-rouge">feature_importances_</code> attribute, which can be used to understand which features are most informative and potentially remove or adjust features that add noise.</li> </ol> <p>Remember, while it’s important to ensure your model doesn’t overfit the training data, it’s equally crucial to ensure it doesn’t underfit by being too simplistic. Balancing complexity and simplicity is key to building a robust decision tree classifier.</p> <h3 id="implementing-k-fold-cross-validation-in-python"> <a href="#implementing-k-fold-cross-validation-in-python" class="anchor-heading" aria-labelledby="implementing-k-fold-cross-validation-in-python"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementing k-fold Cross-Validation in Python </h3> <p>Here’s how you can implement k-fold cross-validation using scikit-learn:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Define the classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Example depth
</span>
<span class="c1"># Use cross_val_score for k-fold cross-validation
</span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># for a 5-fold cross-validation
</span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>

<span class="n">average_score</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Average accuracy across </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s"> folds: </span><span class="si">{</span><span class="n">average_score</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>The advantage of k-fold cross-validation is that it uses the entire dataset for both training and validation, ensuring a more comprehensive evaluation. However, keep in mind that it requires training the model (k) times, which might be computationally expensive for larger datasets or complex models.</p> <h4 id="combining-cross-validation-with-hyperparameter-tuning"> <a href="#combining-cross-validation-with-hyperparameter-tuning" class="anchor-heading" aria-labelledby="combining-cross-validation-with-hyperparameter-tuning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Combining Cross-Validation with Hyperparameter Tuning </h4> <p>When optimizing hyperparameters, such as the depth of a decision tree, it’s beneficial to combine k-fold cross-validation. By doing this, you ensure that the selected hyperparameters are robust across different data subsets. Here’s an example using <code class="language-plaintext highlighter-rouge">GridSearchCV</code> which will search for the best depth while performing k-fold cross-validation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Define the parameter grid
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s">'max_depth'</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)}</span>

<span class="c1"># Create the grid search object with k-fold cross-validation
</span><span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'accuracy'</span><span class="p">)</span>

<span class="c1"># Fit the grid search object to the data
</span><span class="n">grid_search</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Get the best parameters and score
</span><span class="n">best_depth</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span>
<span class="n">best_score</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_score_</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Best depth found using </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">-fold cross-validation: </span><span class="si">{</span><span class="n">best_depth</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Best average accuracy: </span><span class="si">{</span><span class="n">best_score</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>This combined approach will give a more robust estimation of the model’s performance and the optimal hyperparameters, reducing the risk of overfitting on the training data.</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
