{"0": {
    "doc": "Freq-domain Filtering",
    "title": "Freq-domain Filtering",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Frequency-domain Filtering We have discussed how to remove time-domain noise, now let us turn to noise in the frequency domain. ### How frequency domain filtering works A long time ago, French scientist and mathematician Jean Baptiste Fourier (1768–1830) proved the mathematical fact that any periodic waveform can be expressed as the sum of an infinite set of sine waves. The frequencies of these sine waves must be integer multiples of some period. An example of this observation is shown in Figure 4. You take two periodic sine waves, add them up, and you get a complicated looking curve. The inverse is also true. You can take any time-series pattern and break it down into a weighted sum of sinusoidal waves. _Figure 1: What happens if we add a number of sine waves together, with some weights for each wave? We end up with a complicated waveform that is the summation of the individual waves._ What does all of this have to do with smoothing signals? A whole lot as it turns out. The main idea is that noise in these waveforms are often concentrated in some frequencies, and not in others. For example, take the case of the accelerometer walking data in Figure 1. The rate at which you walk is typically one or two steps a second; even if you run, the step rate is a few steps a second. So the frequency of interest is only a few Hz. Similarly, in the case of ECG, the useful frequencies of the electrical signals in the heart are between 0.5 - 150 Hz. In both cases, the frequencies of interest are limited to a relatively small range. Here’s the magic: _once you convert a signal to a weighted sum of sinusoidals, you can just remove all the sinusoidals whose periods are outside the range that you expect, and what you are left with is a much cleaner signal! _This core idea is that by converting data from the time-domain (which we normally look at) to the frequency domain (which is this new way of viewing data as sinusoidals), we can more easily distinguish the useful data from noise and remove the noise. ### Types of Frequency-domain Filters _Figure 2: Types of frequency-domain filters_ Figure 2 shows four types of frequency-domain filters that are commonly used in practice: * **Low-pass filter**: A low-pass filter passes signals with frequency lower than a certain cutoff frequency and attenuates (i.e. reduces the effect of) signals that are higher than the cutoff frequency. An example where a low-pass filter is used is to remove baseline variations in an ECG signal * **High-pass filter**: A high-pass filter passes signals with frequency higher than a cutoff frequency and attenuates signals that are lower than the cutoff. An example where a high-pass filter is used is to remove hand shaking and external vibrations from an ECG signal. * **Bandpass filter**: A bandpass filter allows signals between certain frequencies to pass through but attenuates signals outside this band. An example of use of a bandpass filter is to look at frequencies corresponding to normal walking from an accelerometer signal. * **Notch filter**: A notch filter attenuates the signal within a very small band but lets the other frequencies go through unaltered. An example of use of a notch filter is to remove powerline interference from an ECG signal. In practice, one has to be careful about how to use the above filters. In many practical situations, removing frequencies abruptly causes unwanted artifacts in the signal. For example, some of you might have seen a ringing sound when you listen to percussion instruments in a concert. This topic is more involved and you can learn these in more advanced classes. ### Example: ECG Noise Removal A classic example of how these frequency domain filters are used for noise removal is in the case of ECG filtering, so let us discuss how this works. Figure 8 shows an example of the raw ECG data and the filtered ECG data that we would like to extract from it. There are three sources of noise in the ECG data shown in this picture: _Figure 3: (top) ECG signal with baseline wander, powerline interference, and other high-frequency noise, (bottom) filtered ECG signal._ **Baseline Wander**: Baseline wander is a low-frequency component present in the ECG system which causes the signal to “wander” off from the actual ECG waveform. This is due to offset voltages in the electrodes, due to periodic breathing, and due to body movement. This noise can cause problems in the analysis of the ECG waveform. As you can see in Figure 8, baseline wander is a slowly oscillating waveform, with frequency much lower than the ECG signal that we are interested in. So, intuitively, it can be removed by using a high-pass filter with an appropriately chosen cutoff to remove the baseline wander while letting through the ECG waveform of interest. **Powerline Noise**: The frequency of alternating current in the electrical mains is typically around 50-60Hz. Since this is in the frequency range of the ECG signal that we are interested in, it appears as a significant source of noise that can disrupt any measurement that we wish to make. This noise can be clearly seen in the ECG figure below. The blue curve is the measured ECG signal which has periodic variations of the power line on top of the actual ECG signal in red. Power-line noise can be removed from the ECG signal by implementing a notch filter at 50/60Hz. _Figure 4: (red) ECG signal without powerline noise, and (blue) ECG signal with powerline noise._ **High frequency Noise**: Various other electronic equipment in the vicinity of the ECG sensor including pacemakers, mobile phones, and other electronics can interfere with the ECG signal. These sources of noise are high frequency, and need to be removed by an appropriately selected low-pass filter. If we put these methods together, we get a filtering pipeline that looks like the one below. The ECG signal of interest is between 0.5Hz to 150Hz, so we can remove baseline wander by having a high-pass filter with a cutoff of 0.5Hz, and we can remove high frequency noise by having a low-pass filter with a cutoff of 150Hz. This leaves us with powerline interference, which we can remove with a notch filter with a 50Hz cutoff. _Figure 5: ECG filtering pipeline comprising several frequency-domain filters to cutoff low frequency baseline wander, high frequency RF noise and narrow frequency powerline noise._ ### Implementing Frequency-domain Filters in Python To perform frequency domain filtering on signals, we often use the combination of a design function (to design the filter) and a filtering function to apply it on our data. One common package used for this purpose in Python is `scipy.signal`. In the provided code, the `butter` function from `scipy.signal` is used to design a Butterworth filter, which is a type of IIR (Infinite Impulse Response) filter. The `lfilter` function then applies this filter to a dataset. Let's dive deeper: ```python SAMPLING_RATE = 1000 # Hz # calculate the Nyquist frequency nyq = 0.5 * SAMPLING_RATE # Create filters b_high, a_high = butter(2, 130/nyq, btype='lowpass') # Low pass 40 Hz b_notch, a_notch = butter(2, [59/nyq, 61/nyq], btype='bandstop') # Notch at 60 Hz # Apply filters df['HighPass1'] = lfilter(b_high, a_high, df.Noisy1) df['LowPass2'] = lfilter(b_low, a_low, df.Noisy2) df['Notch3'] = lfilter(b_notch, a_notch, df.Noisy3) ``` #### Understanding the parameters and the role of the sampling rate 1. **SAMPLING_RATE**: This represents how many data points are recorded per second. In digital signal processing, the Nyquist theorem states that a continuous signal can be completely described by its samples and fully reconstructed if it is sampled at a rate at least twice the signal's highest frequency. 2. **butter function**: `butter(N, Wn, btype='low', analog=False, output='ba', fs=None)` is used to design an Nth order Butterworth filter. The function returns two arrays - `b` (numerator) and `a` (denominator) which can be used to construct the transfer function of the filter. - `N`: The order of the filter. - `Wn`: This is the critical frequency or frequencies. This parameter is normalized between 0 and 1, where 1 corresponds to the Nyquist frequency, half of the sampling rate. This is the reason for dividing by `SAMPLING_RATE/2` when specifying the cutoff. - `btype`: Can be ‘lowpass’, ‘highpass’, ‘bandpass’, or ‘bandstop’, to determine the type of filter. - `output`: Specifies the type of output: numerator/denominator (`'ba'`). 3. **lfilter function**: `lfilter(b, a, x)` is used to apply the filter designed by `butter` to a signal `x`. - `b`: The numerator polynomial coefficients of the IIR filter. - `a`: The denominator polynomial coefficients of the IIR filter. - `x`: The signal you want to filter. #### Why the need for the sampling rate in filter design? When designing a digital filter, frequencies are normalized to the Nyquist rate. So, the specification of cutoff or center frequencies in Hz often needs to be converted to a normalized form. This is why the sampling rate is crucial; it provides a context to convert between absolute frequency (in Hz) and normalized frequency. #### Summarizing the provided code - A **low-pass filter** is designed that allows frequencies less than 130Hz. - A **high-pass filter** is designed that allows frequencies greater than 20Hz. - A **notch filter** (or band-stop filter) is designed to attenuate frequencies between 59Hz and 61Hz, which would be useful to remove noise or interference at 60Hz. Each filter is then applied to different noisy datasets using the `lfilter` function. In conclusion, frequency domain filtering is a powerful technique for signal processing, and understanding the role of sampling rates, filter design, and filter application is critical to effectively leveraging this method. ### Example Notebook: Frequency Domain Noise Removal [[html](/mhealth-course/chapter1-noise/notebooks/Chapter1-FreqDomainNoiseRemoval.html)] [[ipynb](/mhealth-course/chapter1-noise/notebooks/Chapter1-FreqDomainNoiseRemoval.ipynb)] This notebook shows a few examples of time-series signals and how different frequency-domain noise removal methods (low-pass, high-pass, and notch filter) work on this data. ",
    "url": "/mhealth-course/chapter1-noise/ch1-freqdomainfiltering.html",
    
    "relUrl": "/chapter1-noise/ch1-freqdomainfiltering.html"
  },"1": {
    "doc": "Intro to Data Smoothing",
    "title": "Intro to Data Smoothing",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Introduction to sensor data smoothing and filtering Most sensor data is affected to some extent by **noise**, that is unexplained variations in the data that in many cases is uninterpretable, and in almost all cases is not of interest to us. Data analysis is often considerably simpler if this noise can be removed from the data. This process is referred to by various terms - data smoothing, filtering, cleaning, and so on. The underlying idea in all these methods is to remove the noise while retaining the important characteristics of the signal. Noise removal techniques can be divided into two class. The first is time-domain approaches, which is the more intuitive way of approaching the problem. The second is frequency-domain approaches, which removes noise that is periodic in nature. Our goal is not to provide a detailed mathematical exposition of these techniques but to keep it practical. You can tackle most issues with noise in sensor data analysis by understanding the general idea underlying these techniques, and by knowing which denoising technique to apply and when. Before we launch into these three classes, however, we give you a brief introduction into the difference being time and frequency domain information in signals, and noise in sensor signals. ## Information and Noise in Signals One very important part of any sensor data processing task is understanding how information is contained in the signals you are working with, and what types of noise can corrupt the signal and make it difficult to extract the information. There are two ways that are common for information to be represented in naturally occurring signals. We will call these: information represented in the time domain, and information represented in the frequency domain. Information represented in the time domain describes when something occurs and what the amplitude of the occurrence is. For example, you may be interested in the temperature of this room, the orientation of your phone, your location and driving trajectory, and so on. All of these contain information in the time domain. In contrast, information represented in the frequency domain is more indirect. Many things in our universe show periodic motion. For example, accelerometer readings when you walk show periodic motion; a wine glass struck with a fingernail will vibrate, producing a ringing sound; your heart beats in a quasi-periodic manner; the pendulum of a grandfather clock swings back and forth, and so forth. By measuring the frequency, phase, and amplitude of this periodic motion, information can often be obtained about the system producing the motion. Suppose we sample the accelerometer on your phone. A single sample, in itself, contains no information about the periodic motion, and therefore no information about the walking activity. The information is contained in the frequency with which patterns corresponding to steps occur in the signal. Noise is anything that corrupts the signal and makes it difficult to extract the information of interest. We deal with noisy signals all the time, often without our knowledge. For example, when we use a thermometer to check body temperature, it is common to take two or three readings to verify that the first one did not have noise. There are many different types of noise --- noise may appear are sudden spikes in the signal, as random perturbations, as periodic variations, and so on. The underlying physical phenomena causing noise can range from quantum noise or electronic and mechanical noise in the sensor, to a variety of external factors that depend on sensor placement, external vibrations and electronic interference and so on. Now that you understand the difference between information and noise, let us turn to what causes noise in sensor data. We will start by showing you different examples of noisy signals and what are some common sources of noise in these signals. ## Noise in Accelerometer Data Figure 1 shows the typical accelerations along the three axes (x, y, and z) while walking. You can see that there is clearly some periodic pattern in this data, but it also seems really noisy making it hard to analyze the data to extract useful information such as number of steps. The noise in the accelerometer signal can be categorized into two types. _Figure 1: Typical pattern of x, y, and z accelerations while walking with smartphone in pocket._ **Intrinsic sensor noise**: Some of the sources of noise in an accelerometer is due to the electronic noise from the circuitry that is converting the motion into a voltage signal and the mechanical noise from the sensor itself. There are several sources of electronic noise, referred to as Johnson noise, shot noise, flicker noise, and several others. The mechanical noise of the sensor comes from thermo-mechanical noise, which arises due to the fact that an accelerometer has tiny moving parts, and these are susceptible to mechanical noise due to molecular agitation. Normally, you don’t have to worry about the intrinsic sensor noise because the sensor manufacturer would have carefully calibrated the sensor and the hardware filters to deal with them. But sometimes they do manifest in the signal. **External vibration noise**: Another common source of noise is external vibration noise, which is noise from the movements external to the sensor and therefore affects the readings. For example, consider the case that you want to detect whether a phone is being carried by a person or not. An intuitive algorithm to determine this would be to check if the accelerometer reading is changing. If the accelerometer reading is continuously changing, that would imply that the phone is being carried, whereas if the reading is not changing, then it is stationary. If you implement this algorithm, you will find that it does not work as expected. The reason is that even though you think that the phone is stationary when placed on say, a table, there are continuous external vibrations induced by the earth’s movement, nearby vehicles, the HVAC system, people’s movement, and many others. These tiny movements manifest as small changes in the accelerometer reading which can be misinterpreted by your algorithm as a person carrying a phone. As another example, take the case of the algorithm that detects how you are holding the phone and changes the screen orientation accordingly. This algorithm can use the accelerometer reading to measure tilt (more on this in a later lecture), but we do not want this measure to be influenced by the tremors of your hand. Even though you don’t realize it, there are continuous small tremors in our hand that will be picked up by an accelerometer. If you do not believe me, try holding a laser pointer in one hand and point it at a wall several meters away to see how steady your hand is. The vibrations of your hand make the accelerometer outputs appear “jittery”. These jitters need to be smoothed before applying an algorithm to determine screen orientation. ## Noise in Electrocardiogram (ECG) Data This problem is by no means isolated to accelerometer signals. For example, if you looked at signal from an ECG sensor, you would see a lot of noise sources such as those shown in Figure 2. ![alt_text](images/image2.png \"image_tooltip\") _Figure 2: Typical ECG signal with different interference sources_ The figure shows four sources of noise in the ECG signal. One visible problem is power line interference i.e. the 50Hz power line signal causes electromagnetic interference which is recorded by the ECG device. This issue is particularly problematic for low frequency signals like ECG. Many other sources of ECG noise are present as well including those caused by breathing, muscle contractions, body movement, and so on. ## Noise in Image Data Images are often noisy, and these sources of noise need to be filtered out before meaningful information can be extracted from the data. Here are two examples of noisy images, and how they look after they are cleaned. In the first example (boat), one could argue that the boat is at least visible in the noisy signal, but in the second example (eye), the noisy data makes it very hard to even identify the fact that it is an image of the eye. Thus, noise can severely impact the visual quality of the image to the point where it may be even hard for the human eye to identify the object(s) present in the image. ![alt_text](images/image10.png \"image_tooltip\") ![alt_text](images/image1.png \"image_tooltip\") ![alt_text](images/image15.png \"image_tooltip\") _Figure 3: Noise in images. (left) salt-and-pepper noise, and (right) fixed pattern noise in camera_ The reasons for image noise are many as well --- often, noise is caused by the camera, especially in poor illumination conditions, high temperature or just electronic noise in the circuit. ## Noise in Audio Signals Audio data recorded by a microphone can also be highly susceptible to noise. The noise could be due to ambient sound, for example, you are speaking in a party where many other people are simultaneously talking. Or it could be due to a loud noise nearby such as talking near a construction site. Of course, the hardware and circuit could add to the noise as well. Below is an example that shows how much noise can distort an audio signal. _Figure 4: Noisy audio signal_ So, how do we deal with noise? Turns out that this is a vast topic, and there are many methods that have been fine tuned to handle noise for different types of sensors. We will not be able to talk about all these methods, but we will try to understand the classes of methods, and the tradeoffs in using these types of filtering techniques. ## Noise in GPS Data Other sensor information like GPS location is also susceptible to noise. GPS readings can be noisy due to a variety of reasons including clock error, tropospheric delays, multipath effects due to buildings, weather conditions, and so on. If you have used Google Maps or any other mapping service, you may be tempted to believe that GPS is fairly accurate, but the raw data coming from the GPS receiver often has noise that is being smoothed before it is displayed on screen. For example, the figure below shows what you would get if you blindly connected the GPS locations coming from your receiver while you are driving down a street. The green line shows the actual trajectory that should have been observed if the GPS readings were error-free. _Figure 5: Noisy GPS readings while driving in red. Actual trajectory in green._ ",
    "url": "/mhealth-course/chapter1-noise/ch1-intro.html",
    
    "relUrl": "/chapter1-noise/ch1-intro.html"
  },"2": {
    "doc": "Sampling and Nyquist",
    "title": "Sampling and Nyquist",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Sampling Rate and Nyquist Frequency Signals in the real world are continuous but for a computer to process the signals, they need to be sampled at a particular rate. For example, the accelerometer on your Fitbit may be sampling at 50Hz i.e. every 0.2 seconds in-order to detect steps. In the figure below, the signal is shown sampled at two different rates to illustrate sampling patterns. _A continuous signal sampled at low sample rate (left) and high sample rate (right)_ **How many samples are necessary to ensure we are preserving the information contained in the signal?** If the signal contains high frequency components, we will need to sample at a higher rate to avoid losing information that is in the signal. In general, to preserve the full information in the signal, it is necessary to sample at twice the maximum frequency of the signal. This is known as the Nyquist rate, which comes from a surprising (but intuitive) result in signal processing that states that a signal can be exactly reproduced if it is sampled at a frequency F, where F is greater than twice the maximum frequency in the signal. For example, imagine we have an ECG signal composed of frequencies between 0 and 200 Hz. To properly digitize this signal, we must sample at 2∗200Hz. So, F needs to be 400Hz. **What happens if we sample the signal at a frequency that is lower that the Nyquist rate?** When the signal is converted back into a continuous time signal, it will exhibit a phenomenon called aliasing. Aliasing is the presence of unwanted components in the reconstructed signal. These components were not present when the original signal was sampled. In addition, some of the frequencies in the original signal may be lost in the reconstructed signal. Aliasing occurs because signal frequencies can overlap if the sampling frequency is too low. Frequencies \"fold\" around half the sampling frequency - which is why this frequency is often referred to as the folding frequency. This is shown in the below figure. **What are the implications of the Nyquist theorem on filtering?** The main implication is that we cannot filter signals that are above the sampling rate. For example, if your data is sampled at 400Hz, then the Nyquist frequency is 200Hz, hence we cannot use a filtering technique to filter out frequencies that are higher than this. _A series of waveforms showing a 7 kHz and 13 kHz tone sampled in a 10 kHz system with the resulting “alias tones” that are generated and output._ To learn more about the Nyquist rate and Aliasing, see [Jack Shaedler's excellent interactive site](https://jackschaedler.github.io/circles-sines-signals/sampling.html). ",
    "url": "/mhealth-course/chapter1-noise/ch1-sampling-nyquist.html",
    
    "relUrl": "/chapter1-noise/ch1-sampling-nyquist.html"
  },"3": {
    "doc": "Time-domain Smoothing",
    "title": "Time-domain Smoothing",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Time-series Smoothing and Filtering We start our discussion of smoothing methods with the simplest method, moving average smoothing and proceed to a variant of this method referred to as exponential weighted smoothing. The fact that they are simple doesn’t mean that they are not useful - in spite of their simplicity, these methods are surprisingly effective in practice, and therefore very widely used. Regardless of the specific smoothing method, they are all applied over rolling or sliding windows as shown below. In other words, a window of data is picked up, then it is smoothed using one of several methods (e.g. moving average), and then the smoothed output sequence is generated. ### Understanding Rolling or Sliding Windows **Concept**: A rolling or sliding window is a subset of data points in a series that \"slides\" across the dataset. The idea is to divide the dataset into many overlapping subsets, perform a computation on each subset, and then move the window by a fixed amount to the next position. Here is a visualization of this procedure. ![alt_text](figures/rolling-window.png \"image_tooltip\") For instance, if we have a dataset: ``` data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] ``` A rolling window of size 3 would generate the following subsets: ``` [1, 2, 3], [2, 3, 4], [3, 4, 5], ..., [8, 9, 10] ``` For each subset, an operation (like averaging for a moving average filter) is performed. The result of this operation on each subset generates a new series. **Why Rolling Windows?**: In time series analysis, data often contains fluctuations or noise that might obscure patterns. Analyzing the data in smaller chunks, or windows, allows us to mitigate the effects of these fluctuations and to discern underlying patterns or trends. Imagine a spotlight moving across a stage, illuminating only a small portion of the stage at a time. As the spotlight moves, you see a changing scene within its confines. Similarly, a rolling window \"illuminates\" a subset of the data, and as it slides, the subset changes, offering a \"localized\" view of the data at each position. **Padding and Edge Effects**: One detail to note when using rolling windows is the \"edge effect\". At the beginning and end of our data series, there aren't enough data points to form a complete window. For example, with our dataset above, the first data point (1) does not have two preceding data points for a window of size 3. One solution to this is \"padding\", where we artificially extend our dataset at the beginning and end. There are different methods to do this: 1. **Zero Padding**: Add zeros to the start or end of the dataset. 2. **Reflective Padding**: Reflect the data points at the edge. For our data, this might mean adding a \"0\" before the \"1\" and an \"11\" after the \"10\". 3. **Constant Padding**: Use the edge values as padding. For our data, this would mean adding another \"1\" before the \"1\" and another \"10\" after the \"10\". The choice of padding method depends on the nature of the data and the specific application. However, padding can introduce inaccuracies, so it's crucial to be aware of the potential implications. ### Moving Average Smoothing One common technique to smooth signals is to perform a moving average. Lets try use an example to illustrate this approach. Lets say we have a noisy accelerometer signal, and let us try to apply a moving average smoothing to this signal. We are going to replace each sample by the average of the current sample, the sample before it, and the sample after it. More precisely, let us represent the input accelerometer signal as follows: x = x1, x2, x3, … xn where the index is the sample number. The output of the moving average filter is: _s1_ = (x1 + x2 + x3)/3 _s2_ = (x2 + x3 + x4)/3 _s3_ = (x3 + x4 + x5)/3 … _sn-2_ = (xn-2 + xn-1 + xn)/3 In the example above, we averaged three input values together, but we could have averaged more nearby points to smooth even more aggressively. As you increase the smoothing window, the signal will look cleaner and more visually pleasing, but beware of using too large a window since you will smooth out the important characteristics of the signal (for example, steps if you want to do step detection). Note that in the above example, we have `n` input samples but `n-2` output samples. This is because there are only `n-2` valid windows of size three in the data. In some cases, though we might want as many output samples as input samples for convenience of processing the data -- you can specify this with appropriate parameters in the python function as described at the end of this page. ### Exponential Moving Average Smoothing Exponential Moving Average (EMA) is another common technique to smooth signals, often used because it gives more weight to recent observations while still considering older observations but with exponentially decreasing weights. This property can be especially useful if you believe that recent observations carry more information about the future than older ones. Let's illustrate this approach with an example similar to the moving average technique. For a window size of 3, we can define weights such that the current sample gets the highest weight, the previous sample gets a lesser weight, and the sample before that gets even lesser weight. More precisely, let us represent the input accelerometer signal as: x = x1, x2, x3, … xn where the index is the sample number. Now, consider weights a, b, and c, with a > b > c and a + b + c = 1. The output of the EMA filter is: _s1_ = a * x1 + b * x2 + c * x3 _s2_ = a * x2 + b * x3 + c * x4 _s3_ = a * x3 + b * x4 + c * x5 … _sn-2_ = a * xn-2 + b * xn-1 + c * xn For instance, if you choose a = 0.5, b = 0.3, and c = 0.2, it ensures that the current sample has more influence than the previous ones. However, this simple illustration does not fully represent the true nature of EMA. In a generalized form, the weight given to each observation decreases exponentially, ensuring that every observation in the dataset has some amount of influence. This is mathematically expressed using a decay factor, often represented as α (alpha). The equation is: _s_ = α * x + (1 - α) * _s_previous Where _s_previous is the previous smoothed value and _s_ is the current smoothed value. The parameter α (alpha) is between 0 and 1, where a higher value gives more weight to recent observations. In libraries like pandas, `span` is often used instead of α, which is an alternative way of defining how much weight the observations should have. Like with the simple moving average, one must take care when choosing the smoothing factor to ensure that important characteristics of the data are not lost. Again, depending on the implementation or use-case, you might want as many output samples as input samples, which can be achieved with appropriate parameters in the Python function, as shown in the example above. The effect of exponential smoothing of an accelerometer signal obtained during walking is shown in Figure 1. You can now start to see the distinct steps much more cleanly, and you can even count them quite easily by eye. _Figure 1: (left) accelerometer signal during walking without smoothing (right) after exponentially weighted smoothing with smoothing = 6_ (i.e. _α = ⅙). ### Median Filtering When the noise appears like sudden spikes in the data (also referred to as salt-and-pepper noise) or if the data has outliers (i.e. spurious readings that are very large or very small compared to the data) or if the data has sharp edges, then the moving average and exponential smoothing methods are not the best methods. An example is shown below, where the noise pattern comprises sharp spikes in the data. Exponential smoothing will remove noise but not very well. _Figure 2: Exponential smoothing vs Median filtering. Median filtering is better for removing spikes in the signal (salt-and-pepper noise) compared to exponential smoothing._ One solution to this issue is to use median filtering. The median filter operates over sliding windows as with moving average and exponential smoothing, but computes the median over each window rather than the average. If the input accelerometer signal is: x = x1, x2, x3, … xn , the output of the median filter is: _s1_ = median(x1 , x2 , x3) _s2_ = median(x2 , x3 , x4) _s3_ = median(x3 , x4 , x5) … _sn-2_ = median(xn-2 , xn-1 , xn) ### Comparison between the three methods When dealing with time-series data from sensors, it's essential to choose the appropriate filtering technique that aligns with the characteristics of the noise and the desired features you wish to preserve in the data. Below are some of the pros and cons of each of the above methods. 1. **Moving Average Smoothing** - **Advantages**: - Simple to implement and understand. - Effective at reducing random noise. - Provides a smoothed curve without much lag, depending on the window size. - **Disadvantages**: - Peaks can be smoothed out, especially with a larger window size. - Sensitive to outliers, as it treats every point with equal weight. - Can introduce a phase lag depending on the size of the window. 2. **Exponential Moving Average (EMA)** - **Advantages**: - Weights recent data more heavily, preserving more recent trends. - Can be tuned (via the alpha parameter) to adjust the level of smoothing versus the lag. - Requires less memory than the moving average as it doesn’t require storing previous observations. - **Disadvantages**: - Still can be influenced by significant outliers, although to a lesser extent than the moving average. - The choice of alpha can be critical and may require iterative experimentation. 3. **Median Filtering** - **Advantages**: - Excellent at preserving edges (sudden changes in data). - Highly effective against salt-and-pepper noise (random high and low spikes). - Not influenced by outliers, making it resistant to skewed data. - **Disadvantages**: - Might not smooth data as uniformly as the other methods. - Requires sorting, which might not be efficient for large window sizes. ### Python Code Examples using Pandas To filter sensor data in Python, the Pandas library is a powerful tool. Below are code snippets for each of the three filtering techniques using Pandas: ```python import pandas as pd import numpy as np # Create sample data data = {'Sensor': np.random.randn(100)} df = pd.DataFrame(data) # Moving Average Smoothing df['Moving_Avg'] = df['Sensor'].rolling(window=3).mean() # Exponential Moving Average df['Exp_MA'] = df['Sensor'].ewm(span=5, adjust=False).mean() # Median Filtering df['Median_Filter'] = df['Sensor'].rolling(window=3).median() ``` **Parameters Explanation**: - For `rolling()`: - `window`: Defines the number of observations to consider. It can be adjusted depending on the desired level of smoothing. The result size depends on the method parameter: - `valid` - Returns only those convolution results that are computed without padding. This shrinks the output size. - `full` - Uses padding, resulting in an output size as large as the input. Edge windows are calculated using the available values, and the rest are filled using padding. - Default is `None` which means the same size as input. - For `ewm()`: - `span`: Defines the span of the Exponential Moving Average i.e. how many samples to consider. Below is an example of rolling window. ![alt_text](images/rolling-window.png \"image_tooltip\") ### Alternate Methods using Numpy You can also use Numpy's `convolve` function for moving average: ```python window = 3 weights = np.repeat(1.0, window) / window sma = np.convolve(df['Sensor'], weights, 'valid') ``` **Parameters Explanation for Numpy's `convolve`**: - The third parameter to `convolve` specifies the mode: - `valid` - Returns only those convolution results that are computed without any padding. This shrinks the output size. - `full` - Uses padding, resulting in an output size larger than the input. Edge windows are calculated using the available values, and the rest are filled using padding. - `same` - Returns convolution of the same size as the input, achieved by using adequate padding. Remember that each method has its strengths and weaknesses, and it's essential to understand the underlying noise characteristics and the features of interest in your data before choosing a filter. It's also a good practice to visualize the filtered data to ensure it aligns with your expectations. Because they are so simple to implement and understand, time-domain smoothing is often the first methods tried when faced with a problem. These work well in practice when noise is in the time domain but many sensor signals have frequency domain noise, so it is important not to rely too much on time domain smoothing. ### Example Notebook: Time Domain Noise Removal [[html](/mhealth-course/chapter1-noise/notebooks/Chapter1-TimeDomainNoiseRemoval.html)] [[ipynb](/mhealth-course/chapter1-noise/notebooks/Chapter1-TimeDomainNoiseRemoval.ipynb)] This annotated notebook shows a few examples of time-series signals and how different time-domain smoothing methods (moving average, exponentially weighted moving average, and median filtering) work on this data. ",
    "url": "/mhealth-course/chapter1-noise/ch1-timedomainfiltering.html",
    
    "relUrl": "/chapter1-noise/ch1-timedomainfiltering.html"
  },"4": {
    "doc": "Counting Calories",
    "title": "Counting Calories",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Counting Calories Determining the exact number of calories expended by an individual based on the number of steps that they have taken is an approximate science, and there is no accurate means of obtaining this measure. However, there are some useful heuristics that can be applied, and we are going to explore one of them. The basic idea is to figure out the running/walking speed of the individual (distance/time), and then use information about the weight to determine calories expended. **Estimating Distance Covered**: After computing the steps parameter according to the algorithm above, we can use Equation 1 to get the distance parameter. _Distance = number of steps × distance per step (1)_ Distance per step depends on the speed and the height of user. The step length would be longer if the user is taller or running at higher speed. We use the steps counted in every two seconds to judge the current stride length. Table 2 shows the experimental data used to judge the current stride. _Table 2. Stride as a Function of Speed (steps per 2 s) and Height_ **Estimating Speed**: Speed = distance/time, so Equation 2 can be used to get the speed parameter, as steps per 2 s and stride have all been calculated according to the algorithm above. _Speed = steps per 2 s × stride/2 s (2)_ **Estimating Calories**: Unfortunately, there is no accurate means for calculating the rate of expending calories. Some factors that determine it include body weight, intensity of workout, conditioning level, and metabolism. We can estimate it using a conventional approximation, however. Table 3 shows a typical relationship between calorie expenditure and running speed. _Table 3. Calories Expended vs. Running Speed_ From Table 3, we can get (3). _Calories (C/kg/h) = 1.25 × running speed (km/h) (3)_ The unit of the speed parameter used above is m/s; converting km/h to m/s gives Equation 4. _Calories (C/kg/h) = 1.25 × speed (m/s) × 3600/1000 = 4.5 × speed (m/s) (4)_ The calories parameter would be updated every 2s with the distance and speed parameters. So, to account for a given athlete’s weight, we can convert Equation 4 to Equation 5 as indicated. Weight (kg) is a user input, and one hour is equal to 1800 two second intervals. _Calories (C/2 s) = 4.5 × speed × weight/1800 = speed × weight/400 (5)_ If the user takes a break in place after walking or running, there would be no change in steps and distance, speed should be zero, then the calories expended can use Equation 6 since the caloric expenditure is around 1 C/kg/hour while resting. _Calories (C/2 s) = 1 × weight/1800 (6)_ Finally, we can add calories for all 2-second intervals together to get the total calories expended. ## References [1] [Full-Featured Pedometer Design Realized with 3-Axis Digital Accelerometer](http://www.analog.com/static/imported-files/tech_articles/pedometer.pdf), Neil Zhao \\ [2] [Step Detection Algorithms for Accelerometers](http://nitarc.be/map/paper/AMBIT_ThuerVerwimp.pdf), Guillaume Thuer and Tim Verwimp \\ [3] [Calorie counting calculation](http://www.livestrong.com/article/80988-caloric-intake-body-mass/), Beth Spicer \\ [4] [A simple method for reliable footstep detection on embedded sensor platforms](http://ubicomp.cs.washington.edu/uwar/libby_peak_detection.pdf), Ryan Libby \\ [5] [A pedometer in the real world](http://www.aosabook.org/en/500L/a-pedometer-in-the-real-world.html), Dessy Daskalov ",
    "url": "/mhealth-course/chapter2-steps/ch2-calories.html",
    
    "relUrl": "/chapter2-steps/ch2-calories.html"
  },"5": {
    "doc": "Intro to Step Counting",
    "title": "Intro to Step Counting",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Introduction Pedometers or step counters are now popular as an everyday exercise progress monitor and motivator. The increasing popularity of these devices can be attributed to several reasons. First, many people are known to overestimate their level of activity, hence these devices can provide more reliable feedback to an individual about how much or little they move during the day. Second, they provide instant and constant feedback about activity levels, making it possible to “gamify” by providing credits for every step an individual takes. Third, they can encourage individuals to compete with themselves in getting fit and losing weight. _Figure 1: Definition of each axis._ ## Overview From the characteristics that can be used to analyze running or walking, we choose _acceleration _as the relevant parameter. The three components of motion for an individual (and their related axes) are forward (_roll_), vertical (_yaw_), and side (_pitch_), as shown in Figure 1. The 3-axis accelerometer senses acceleration along its three axes: _x_, _y_, and _z_. The pedometer will be in an unknown orientation, so the measurement accuracy should not depend critically on the relationship between the motion axes and the accelerometer’s measurement axes. _Figure 2: Walking stages and acceleration pattern_ Let’s think about the nature of walking. Figure 1 depicts a single step, defined as a unit cycle of walking behavior, showing the relationship between each stage of the walking cycle and the change in vertical and forward acceleration. Figure 2 shows a typical pattern of x-, y-, and z- measurements corresponding to vertical, forward, and side acceleration of a running person. The figure should give you confidence that you’re on the right track - the large spikes corresponding to each step, and the periodic nature of walking/running, suggest that we should be able to detect these patterns. _Figure 3: Step counting in an ideal situation._ In an ideal situation, you would like to see a signal like shown in Figure 3. One of the axes of the phone is along the direction of gravity, and the steps can be clearly observed in the signal. But Figure 3 is misleading - if you take a few traces with the phone oriented in different ways in your pocket, you will quickly realize that the shape of the x, y, z curves depends on the orientation of the phone! While the figure shows the y-axis acceleration to be the largest, this may not be the case if the x-axis is oriented downward. _Figure 4: Accelerometer signal in a real-world situation_ A more realistic signal is shown in Figure 4, where some component of the user acceleration and gravity is present along all three axes. Since acceleration changes as a result of a step can result in changes along all the three axes, so we need to design an _orientation-independent_ algorithm to detect steps. ",
    "url": "/mhealth-course/chapter2-steps/ch2-intro.html",
    
    "relUrl": "/chapter2-steps/ch2-intro.html"
  },"6": {
    "doc": "Step Detection Algorithm",
    "title": "Step Detection Algorithm",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Step Detection Algorithm There are many different ways that we can design a step detection algorithm. We outline one such method in this section. The key insight in our method is to convert the 3-axis signal into a one axis magnitude signal, and then extract steps from this signal. _Figure 5: Step Detection Algorithm_ **Step 1: Extract signal magnitude**: In the previously described algorithm, we selected the axis along which maximum acceleration occurred and focused on that one. Here, we are just going to take the magnitude of the entire acceleration vector i.e. , where x, y, and z are the readings of the accelerometer along the three axes. _Figure 6: Example showing sources of noise in magnitude signal_ **Step 2: Filter the signal to remove noise**: The second step is to remove noise, and extract the specific signal corresponding to walking. Before we perform this step, we need to know what are the sources of noise. There are several sources of noise that we need to filter out (shown in Figure 6): * **Jumpy peaks**: Since the phone is often carried in a pocket/purse, it can jiggle a little with each step. Also, some users have a bounce in their step, so even though they are taking a single step, the phone can bounce multiple times within this step. * **Short peaks**: Small peaks can occur when a user is using a phone (e.g. making a call or using an app). * **Slow peaks**: Slow peaks can occur when the phone is moved or due to movements of the leg while sitting (if the phone is in the pant pocket) To remove these sources of noise, we are going to use frequency-domain noise removal. Notice that we need to remove high frequency variations like jumpy peaks and low frequency variations like slow peaks. A simple solution is to use a filter that keeps only frequencies relating to walking and removes the rest. For example, we know that typical walking pace may be under three steps a second (3 Hz) and over half step a second (0.5Hz), so perhaps we remove all frequencies above 5 Hz and below 0.5 Hz (just to give some margin for error). Note that this method would not be able to detect running or bicycling, which may have higher pace. Even after we remove low and high frequency peaks, we may be left with some short peaks. A simple way to deal with this is to look only for large peaks and ignore small peaks. _Figure 7: Zero crossings (left) and peaks (right) of the filtered magnitude signal_ **Step 3: Detecting Steps.** Once you have the smoothed data, let us consider how to detect the step. There are many approaches to do this. We could do what was suggested earlier, which is to look for large peaks and use that to detect steps. Another approach is to take the derivative (slope) of the smoothed acceleration signal. The derivative changes from negative to positive (or positive to negative) exactly when a step occurs, so you can just count the number of times the derivative changed from negative to positive to detect the number of steps that occurred. Another possibility is to subtract the mean for each window and look at zero crossings i.e. times when the signal crosses from the negative to positive in the upward direction (this can be tricky, however, since the signal baseline can change over time as shown below). We will focus on detecting peaks using Python and tuning parameters to make it work effectively. ## Implementing Step Counting in Python Step counting, at its core, is about detecting repeating patterns or peaks in acceleration data that correspond to an individual's steps. In Python, the `scipy` library provides the `find_peaks` function that serves precisely this purpose, allowing us to detect peaks in our dataset easily. ## The `find_peaks` Function The `find_peaks` function from the `scipy.signal` module is designed for pinpointing the indices of relative maxima (peaks) in a 1D array. Its standard usage is: ```python from scipy.signal import find_peaks peaks, properties = find_peaks(data_array, height=ht, prominence=prom, distance=dist, width=wid) ``` For this function: - `data_array` is the time-series dataset where we aim to detect peaks. - `height` serves as a threshold that peaks must surpass for detection. - `prominence` designates how elevated a peak is in relation to its neighbors, emphasizing the peak's relative prominence. - `distance` is the minimum horizontal separation (in data points) expected between peaks. - `width` refers to the width of the peaks at half-prominence. Note that for the assignment, we primarily ask you to work with `distance` rather than the other parameters. For our step counting scenario: ```python peaks, _ = find_peaks(df['accel_mag'], height=ht, prominence=prom, distance=dist, width=wid) num_steps = len(peaks) ``` Here, we're looking for peaks in the `accel_mag` column of our DataFrame, which symbolizes the magnitude of acceleration data. By counting these peaks, we get an estimate of the steps taken. However, without careful parameter tuning, this estimate can differ significantly from the true value. ### Tuning Parameters: Height, Prominence, Distance, and Width - **Height:** This threshold ensures only peaks exceeding a certain value are detected, helping to filter out minor fluctuations and zeroing in on significant movements. - **Prominence:** Useful in discerning genuine peaks from mere noise. A heightened prominence value ensures only peaks distinctly pronounced from their surroundings are identified. This precision is important for sidestepping minor data disturbances being misconceived as steps. - **Distance:** Crucial for step detection, the `distance` parameter corresponds to our understanding of the time lapse between two successive steps. For example, during regular walking, we usually register 1-2 steps every second. Adjusting the `distance` parameter helps in preventing the recognition of multiple peaks within a single step's duration. - **Width:** The `width` parameter captures the full width of a peak at its half-prominence. This becomes particularly relevant in discerning between short spikes (possibly noise or artifacts) and genuine peaks of activity, like steps. In our context, `width` can reflect the typical duration of a step, and filtering peaks based on this duration can improve accuracy. ### The Role of Sampling Rate Sampling rate, denoted as the number of samples gathered each second, is a cornerstone in peak detection. Given our earlier example of a 1-2 step walking rate: - With a 50 Hz sampling rate, a step might span 25 to 50 samples. - At a 100 Hz sampling rate, a step could range from 50 to 100 samples. Clearly, the optimal values for parameters, especially `distance` and `width`, will vary with the sampling rate. Thus, when adjusting these parameters for `find_peaks`, it's crucial to keep the sampling rate of your data in mind to ensure precise peak (step) detection. ## Notebook: Step Counting with Find Peaks [[html](/mhealth-course/chapter2-steps/notebooks/Chapter2-StepCounting.html)] [[ipynb](/mhealth-course/chapter2-steps/notebooks/step-counting.zip)] This notebook shows a step counter using `find_peaks` and applies it to a number of sample sensor logs. The different logs correspond to different sensor placements (left pocket, right pocket, wrist), and to different walking patterns (e.g. with delays between short burst of steps). The notebook shows how tweaking the `prominence` and `width` parameters can allow you to fine-tune the performance of the step counter. ",
    "url": "/mhealth-course/chapter2-steps/ch2-stepcounter.html",
    
    "relUrl": "/chapter2-steps/ch2-stepcounter.html"
  },"7": {
    "doc": "Evaluating Classifier Performance",
    "title": "Table of Contents",
    "content": ". | Chapter 4: Evaluating Classifier Performance . | Understanding the Confusion Matrix | True Positives, True Negatives, False Positives, and False Negatives | Metrics Derived from the Confusion Matrix . | Precision for ‘Sitting (A)’: | Recall for ‘Sitting (A)’: | F1 Score for ‘Sitting (A)’: | . | Caveat: Data Imbalance (Class Skewness) | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-classifier-performance.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-classifier-performance.html#table-of-contents"
  },"8": {
    "doc": "Evaluating Classifier Performance",
    "title": "Chapter 4: Evaluating Classifier Performance",
    "content": "So far, we have seen how to construct a decision tree classifier. But how do we ensure that our decision tree classifier (or any classifier) is up to the task? This chapter delves into the world of classifier evaluation. Understanding the Confusion Matrix . At the heart of evaluating classifiers is the confusion matrix. It offers a snapshot of how a classifier performed, providing insights into both correct predictions and different types of errors. A confusion matrix for our three-class problem (Sitting, Standing, and Walking) can be visualized as: . | Actual \\ Predicted | Sitting (A) | Standing (B) | Walking (C) | . | Sitting (A) | 25 | 5 | 2 | . | Standing (B) | 3 | 28 | 1 | . | Walking (C) | 1 | 0 | 30 | . The rows of this matrix represent the actual classes, while the columns represent the predicted classes. True Positives, True Negatives, False Positives, and False Negatives . To better interpret the matrix, we need to understand a few essential terms: . | True Positive (TP): Refers to the cases where the classifier correctly predicts a positive outcome. For instance, in our example, there are 25 TPs for Sitting (A) where the classifier correctly predicted ‘Sitting’ when the actual activity was indeed ‘Sitting’. | True Negative (TN): Refers to the cases where the classifier correctly predicts a negative outcome. Considering the ‘Sitting’ class, a TN would be a situation where an instance was correctly predicted as ‘not Sitting’. | False Positive (FP): It’s when the classifier incorrectly predicts a positive outcome. For instance, if the actual activity was ‘Sitting’, but the classifier predicted ‘Walking’, it has committed a false positive error for the ‘Walking’ class. | False Negative (FN): It’s when the classifier incorrectly predicts a negative outcome. If the actual activity was ‘Walking’ but the classifier predicted ‘Sitting’, it has made a false negative error for the ‘Walking’ class. | . Visual Example: . Let’s break down the ‘Walking’ activity in our matrix: . | 30 instances were correctly classified as Walking (C) – these are True Positives. | 1 instance of Standing (B) was mistakenly classified as Walking (C) – a False Positive for the ‘Walking’ detection. | 2 instances were actually Sitting (A) but were mistakenly classified as Walking (C) – these are also False Positives for the ‘Walking’ detection. | Instances where the actual activities were either Sitting (A) or Standing (B) and were not classified as Walking (C) contribute to True Negatives for ‘Walking’. | . Metrics Derived from the Confusion Matrix . Accuracy: Represents the overall correctness of the classifier. [ \\text{Accuracy} = \\frac{\\text{Sum of correct classifications (diagonal values)}}{\\text{Total number of classifications}} ] . Number of Correct Predictions (Diagonal elements): 25 + 28 + 30 = 83 Total Predictions: Sum of all elements in the matrix = 95 . [ \\text{Accuracy} = \\frac{83}{95} = 0.8737 ] or 87.37\\% . While Accuracy is calculated across all classes, the bellow three metrics i.e. Precision, Recall, and F1 Score, are typically defined for each class, especially in multi-class classification problems. Precision: Given that a specific class was predicted, how often was that prediction correct? . [ \\text{Precision}_A = \\frac{\\text{tp}_A}{\\text{tp}_A + \\text{fp}_A} ] Where tp (true positive) is the correct prediction count for that class, and fp (false positive) is the count of other classes incorrectly predicted as that class. For Sitting (A), Precision would be: . [ \\text{Precision}_A = \\frac{25}{25 + 3 + 1} \\approx 0.86 ] . Recall (Sensitivity): Of all the instances of a specific class in the dataset, how many were correctly predicted by the classifier? . [ \\text{Recall}_A = \\frac{\\text{tp}_A}{\\text{tp}_A + \\text{fn}_A} ] Where fn (false negative) is the count of instances of that class incorrectly predicted as another class. For Sitting (A), Recall would be: . [ \\text{Recall}_A = \\frac{25}{25 + 5 + 2} \\approx 0.78 ] . F-measure (F1 Score): A balanced harmonic mean of Precision and Recall, helpful when you want to balance the two metrics and have a single performance number. [ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} ] . To get the F1 Score, we first need the precision and recall for each class. Let’s compute it for the class ‘Sitting (A)’: . Precision for ‘Sitting (A)’: . [ \\text{Precision}_A = \\frac{\\text{TP}_A}{\\text{TP}_A + \\text{FP}_A} = \\frac{25}{25 + 3 + 1} = \\frac{25}{29} = 0.8621 ] . Recall for ‘Sitting (A)’: . [ \\text{Recall}_A = \\frac{\\text{TP}_A}{\\text{TP}_A + \\text{FN}_A} = \\frac{25}{25 + 5 + 2} = \\frac{25}{32} = 0.7813 ] . F1 Score for ‘Sitting (A)’: . [ \\text{F1 Score}_A = 2 \\times \\frac{0.8621 \\times 0.7813}{0.8621 + 0.7813} = 0.8197 ] . Similarly, you can calculate the precision, recall, and F1 Score for classes ‘Standing (B)’ and ‘Walking (C)’. With these evaluation metrics in hand, you can rigorously assess your decision tree classifier and understand its strengths and potential areas of improvement. Caveat: Data Imbalance (Class Skewness) . Often, the dataset used to evaluate a classifier might not have an even distribution of classes. Imagine a scenario where you have 1000 instances of ‘Walking’ but only 50 instances of ‘Sitting’ and 50 instances of ‘Standing’. This creates a data imbalance. Problems with Data Imbalance: . | Biased Classifier: The classifier might tend to favor classes with a higher number of instances. It could achieve a high accuracy simply by predicting the majority class for all inputs, leaving the minority class poorly classified. | Misleading Accuracy: With skewed data, a high accuracy might not indicate a well-performing classifier. For instance, a naive classifier that always predicts ‘Walking’ would achieve an accuracy of approximately 91\\% in the skewed dataset above. Yet, it would be entirely failing to identify ‘Sitting’ and ‘Standing’ instances. | . When faced with imbalanced datasets, it’s essential to use metrics beyond just accuracy. Precision, recall, and F1 score give a clearer picture of classifier performance. The implications of these errors can vary based on the application. In some contexts, a false positive might be more problematic than a false negative, and vice-versa. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-classifier-performance.html#chapter-4-evaluating-classifier-performance",
    
    "relUrl": "/chapter3-activityrecognition/ch3-classifier-performance.html#chapter-4-evaluating-classifier-performance"
  },"9": {
    "doc": "Evaluating Classifier Performance",
    "title": "Evaluating Classifier Performance",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-classifier-performance.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-classifier-performance.html"
  },"10": {
    "doc": "Cardiac Risk Prediction",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree-example.html#human-activity-recognition"
  },"11": {
    "doc": "Cardiac Risk Prediction",
    "title": "Table of Contents",
    "content": ". | Example: Cardiac Risk Prediction . | Cardiac Risk Prediction Dataset | Initial Entropy Calculation | Information Gain Calculation . | For Age | For Smoker | For Exercise | . | Dataset subset for “Smoker = Yes” . | For Age: | . | Weighted Entropy and Information Gain for Exercise | Dataset subset for “Smoker = No” . | For Age | . | Weighted Entropy and Information Gain for Exercise | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree-example.html#table-of-contents"
  },"12": {
    "doc": "Cardiac Risk Prediction",
    "title": "Example: Cardiac Risk Prediction",
    "content": "Suppose our dataset contains the following attributes: . | Age: &lt;40, 40-55, &gt;55 | Smoker: Yes, No | Exercise: Regularly, Rarely | . Our target variable is: Risk: High, Low . Cardiac Risk Prediction Dataset . | Age | Smoker | Exercise | Risk | . | &lt;40 | No | Regularly | Low | . | &lt;40 | No | Rarely | Low | . | &lt;40 | No | Rarely | Low | . | &lt;40 | Yes | Regularly | Low | . | &lt;40 | Yes | Rarely | High | . | 40-55 | No | Regularly | Low | . | 40-55 | No | Rarely | High | . | 40-55 | Yes | Regularly | High | . | 40-55 | Yes | Rarely | High | . | &gt;55 | No | Regularly | Low | . | &gt;55 | Yes | Regularly | High | . | &gt;55 | Yes | Rarely | High | . | &gt;55 | No | Rarely | High | . Initial Entropy Calculation . From the table: . | $p_{\\text{Low}} = \\frac{6}{13}$ | $ p_{\\text{High}} = \\frac{7}{13} $ | . Thus, the entropy for this set $ S $ is: $ \\text{Entropy}(S) = -\\frac{6}{13} \\log_2(\\frac{6}{13}) - \\frac{7}{13} \\log_2(\\frac{7}{13}) \\approx 1$ . Information Gain Calculation . For Age . | Age &lt;40: . | Total: 5 | High Risk: 1 | Low Risk: 4 | $ \\text{Entropy} = -\\frac{1}{5} \\log_2(\\frac{1}{5}) - \\frac{4}{5} \\log_2(\\frac{4}{5}) $ $ \\approx 0.72 $ | . | Age 40-55: . | Total: 4 | High Risk: 3 | Low Risk: 1 | $ \\text{Entropy} = -\\frac{3}{4} \\log_2(\\frac{3}{4}) - \\frac{1}{4} \\log_2(\\frac{1}{4}) \\approx 0.81$ | . | Age &gt;55: . | Total: 4 | High Risk: 3 | Low Risk: 1 | $ \\text{Entropy} \\approx 0.81 \\text{(same as above)}$ | . | . Let us compute the weighted entropies and then the information gain for the Age attribute in the next step. $ \\text{Weighted Entropy for Age} = \\frac{5}{13} \\times 0.72 + \\frac{4}{13} \\times 0.81 + \\frac{4}{13} \\times 0.81 \\approx 0.78$ . $ \\text{Information Gain for Age} = \\text{Initial Entropy} - \\text{Weighted Entropy for Age} \\approx 1 - 0.78 \\approx 0.22$ . For Smoker . | Smoker = Yes: . | Total: 6 | High Risk: 5 | Low Risk: 1 | $ \\text{Entropy} = -\\frac{5}{6} \\log_2(\\frac{5}{6}) - \\frac{1}{6} \\log_2(\\frac{1}{6}) \\approx 0.65 $ | . | Smoker = No: . | Total: 7 | High Risk: 2 | Low Risk: 5 | $\\text{Entropy} = -\\frac{2}{7} \\log_2(\\frac{2}{7}) - \\frac{5}{7} \\log_2(\\frac{5}{7}) \\approx 0.86$ | . | . $ \\text{Weighted Entropy for Smoker} \\approx \\frac{6}{13} \\times 0.65 + \\frac{7}{13} \\times 0.86 \\approx 0.76 $ . $ \\text{Information Gain for Smoker} = \\text{Initial Entropy} - \\text{Weighted Entropy for Smoker} \\approx 1 - 0.76 \\approx 0.24 $ . For Exercise . | Exercise = Regularly: . | Total: 6 | High Risk: 2 | Low Risk: 4 | $ \\text{Entropy} = -\\frac{4}{6} \\log_2(\\frac{4}{6}) - \\frac{2}{6} \\log_2(\\frac{2}{6}) = 0.92 $ | . | Exercise = Rarely: . | Total: 7 | High Risk: 5 | Low Risk: 2 | $ \\text{Entropy} = -\\frac{2}{7} \\log_2(\\frac{2}{7}) - \\frac{5}{7} \\log_2(\\frac{5}{7}) = 0.86 $ | . | . $ \\text{Weighted Entropy for Exercise} = \\frac{6}{13} \\times 0.92 + \\frac{7}{13} \\times 0.86 \\approx 0.89 $ . $ \\text{Information Gain for Exercise} = \\text{Initial Entropy} - \\text{Weighted Entropy for Exercise} \\approx 1 - 0.89 \\approx 0.11 $ . Given the information gains: . | Age: 0.22 | Smoker: 0.24 | Exercise: 0.11 | . The attribute Smoker has the highest information gain and will be the root node. For the second-level decision, let’s take the subset of data for one of the age groups, say “Age &lt; 40”, and repeat the computation of information gain for “Smoker” and “Exercise”. We’ll select the attribute with the highest information gain to split the data further. Dataset subset for “Smoker = Yes” . | Age | Smoker | Exercise | Risk | . | &lt;40 | Yes | Regularly | Low | . | &lt;40 | Yes | Rarely | High | . | 40-55 | Yes | Regularly | High | . | 40-55 | Yes | Rarely | High | . | &gt;55 | Yes | Regularly | High | . | &gt;55 | Yes | Rarely | High | . From the subset, the probabilities are: . | $ p_{\\text{High}} = \\frac{5}{6} = 0.83 $ | $ p_{\\text{Low}} = \\frac{1}{6} = 0.17 $ | . Thus, the entropy for this subset $ S $ is: $-\\frac{5}{6} \\log_2(\\frac{5}{6}) - \\frac{1}{6} \\log_2(\\frac{1}{6}) $ $ \\approx 0.65 $ . For Age: . | Age &lt;40: . | Total: 2 | High Risk: 1 | Low Risk: 1 | $ \\text{Entropy} = -\\frac{1}{2} \\log_2(\\frac{1}{2}) - \\frac{1}{2} \\log_2(\\frac{1}{2}) = 1 $ | . | Age 40-55: . | Total: 2 | High Risk: 2 | Low Risk: 0 | $ \\text{Entropy} = 0$ | . | Age &gt;55: . | Total: 2 | High Risk: 2 | Low Risk: 0 | $ \\text{Entropy} = 0 $ | . | . $ \\text{Weighted Entropy for Age} = \\frac{2}{6} \\times 1 + \\frac{2}{6} \\times 0 + \\frac{2}{6} \\times 0 \\approx 0.33$ . $ \\text{Information Gain for Age} = \\text{Initial Entropy} - \\text{Weighted Entropy for Age} \\approx 0.65 - 0.33 \\approx 0.32$ . Weighted Entropy and Information Gain for Exercise . | Exercise = Regularly: . | Total: 3 | High Risk: 2 | Low Risk: 1 | $ \\text{Entropy} = -\\frac{2}{3} \\log_2(\\frac{2}{3}) - \\frac{1}{3} \\log_2(\\frac{1}{3}) = 0.92 $ | . | Exercise = Rarely: . | Total: 3 | High Risk: 3 | Low Risk: 0 | $ \\text{Entropy} = 0 $ | . | . $ \\text{Weighted Entropy for Exercise} = \\frac{3}{6} \\times 0.92 + \\frac{3}{6} \\times 0 = 0.46 $ $ \\text{Information Gain for Smoker} = 0.65 - 0.46 = 0.19 $ . Given the information gains for the “Age &gt; 55” group: . | Age: 0.32 | Exercise: 0.19 | . “Age” has the higher Information Gain for this group, so we will choose this as the next level node for Smoker=Yes. Dataset subset for “Smoker = No” . | Age | Smoker | Exercise | Risk | . | &lt;40 | No | Regularly | Low | . | &lt;40 | No | Rarely | Low | . | &lt;40 | No | Rarely | Low | . | 40-55 | No | Regularly | Low | . | 40-55 | No | Rarely | High | . | &gt;55 | No | Regularly | Low | . | &gt;55 | No | Rarely | High | . From the subset, the probabilities are: . | $ p_{\\text{High}} = \\frac{2}{7} = 0.29 $ | $ p_{\\text{Low}} = \\frac{5}{7} = 0.71 $ | . Thus, the entropy for this subset $ S $ is: $-\\frac{2}{7} \\log_2(\\frac{2}{7}) - \\frac{5}{7} \\log_2(\\frac{5}{7}) $ $ \\approx 0.86 $ . For Age . | Age &lt;40: . | Total: 3 | High Risk: 0 | Low Risk: 3 | $ \\text{Entropy} = 0 $ | . | Age 40-55: . | Total: 2 | High Risk: 1 | Low Risk: 1 | $\\text{Entropy} = 1$ | . | Age &gt;55: . | Total: 2 | High Risk: 1 | Low Risk: 1 | $\\text{Entropy} = 1$ | . | . $ \\text{Weighted Entropy for Age} = \\frac{3}{7} \\times 0 + \\frac{2}{7} \\times 1 + \\frac{2}{7} \\times 1 \\approx 0.57$ . $ \\text{Information Gain for Age} = \\text{Initial Entropy} - \\text{Weighted Entropy for Age} \\approx 0.86 - 0.57 \\approx 0.29$ . Weighted Entropy and Information Gain for Exercise . | Exercise = Regularly: . | Total: 3 | High Risk: 0 | Low Risk: 3 | $ \\text{Entropy} = 0 $ | . | Exercise = Rarely: . | Total: 4 | High Risk: 2 | Low Risk: 2 | $ \\text{Entropy} = 1 $ | . | . $ \\text{Weighted Entropy for Exercise} = \\frac{3}{7} \\times 0 + \\frac{3}{7} \\times 1 = 0.43 $ $ \\text{Information Gain for Smoker} = 0.86 - 0.43 = 0.43 $ . Given the information gains for the “Smoker=No” group: . | Age: 0.29 | Exercise: 0.43 | . “Exercise” has the higher Information Gain for this group, so we will choose this as the next level node for Smoker=No. Thus, the structure of our decision tree becomes: . | Root node: Smoker . | For Smoker=Yes: Split based on Age. | For Smoker-No: Split based on Exercise. | . | . To determine the final classification under each branch, you would continue the process of calculating Information Gain until you reach leaves with maximum purity (i.e. all High or all Low) or until you run out of attributes to split on. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html#example-cardiac-risk-prediction",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree-example.html#example-cardiac-risk-prediction"
  },"13": {
    "doc": "Cardiac Risk Prediction",
    "title": "Cardiac Risk Prediction",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree-example.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree-example.html"
  },"14": {
    "doc": "Decision Tree Classifier",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree.html#human-activity-recognition"
  },"15": {
    "doc": "Decision Tree Classifier",
    "title": "Table of Contents",
    "content": ". | Decision tree classifier | Decision Tree Construction using Entropy . | Entropy | . | Multiclass Entropy . | Information Gain | Building the Tree: C4.5 Algorithm | . | Implementing Decision Trees in Python . | About the DataFrame | Implementing the Decision Tree using Scikit-learn | Parameters of the DecisionTreeClassifier | . | . Decision tree classifier . Once we extract different features from the data, it’s time to build our classifier. At a high level, the goal of a classifier is to identify which of the above features that you obtained from your raw data is most useful in distinguishing between the different activities that you want to classify. There are many different classifiers, and an exhaustive summary would take too much time. But let me try to introduce one such classifier to provide an intuition for how such a method would work. What is a decision tree? One of the most commonly used classifiers is a decision tree. The idea is simple and best explained with an example. Suppose you had the following six features - means along each of the x, y, z axes, and standard deviations along each of these axes. Given these features, you want to distinguish between standing, sitting, walking, running, biking, and driving. The key question is which of these features is most useful to distinguish between these activities. How does a decision tree work? The idea behind a decision tree is to view this problem in a hierarchical manner. First, let us assume that we already constructed a decision tree (see figure above), and just try to understand how to use it, and what its telling us. In the above figure, the root node makes a decision on whether meanX &lt; 8.48 (this number is not important for now, focus on the concept). If true, then the decision is to take the right branch, and if false, take the left branch. Say we took the right branch - then we look at another decision, say checking if stddevX &lt; 11.36. Similarly, this decision tree proceeds by checking one feature after another until we finally get to the leaf node that tells us what our current state is. So, the process of using a decision tree to classify the current activity seems intuitive - its just a sequence of if-then-else statements with each statement checking the values of one or more parameters. But what does each of these branches really mean? What separates the left branch and the right branch? . Figure 3: Example of a decision tree . Decision tree with an example. Let us look at an example of a decision tree and try to interpret its meaning. The root node separates {sitting, standing, driving} on the left branch from {biking, walking, running} on the right branch. Clearly, this node is identifying some feature that separates between more sedentary activities from the more active ones. This makes sense - if we had to think of an algorithm, we would perhaps do the same thing. Let’s now look at the decision nodes at the next level down starting with the left branch. This node separates driving from other sedentary activities (sitting, standing) by looking at the standard deviation of the Z axis. This seems to make sense as well - driving will cause lots of vibrations due to the car and the road, whereas we are unlikely to see these vibrations when you are sitting on a chair or standing (unless there’s an earthquake!). The corresponding decision node on the right branch seems to be doing something similar - it uses standard deviation on the vertical axis to distinguish between biking, and other physical activities such as running and walking. The intuition is that biking on the road is likely to have small vibrations because of the road whereas running or walking has large variations due to acceleration changes for each step (as you observed in the pedometer case study). The remaining decision nodes in the left branch no longer use the standard deviation, and only use the mean to separate sitting and standing. This is easiest to understand from Figure 1 where we saw the raw accelerometer signal for sitting and standing. Both are flat, so there’s little to be gained from looking at standard deviation, but they have different averages, so this is the most useful feature for separating the classes. On the right branch, walking vs running uses the difference in standard deviations between these two - intuition would suggest that running has more vibrations than walking. In summary, a decision tree works by identifying which features best separate the categories, and builds it as a tree structure. Perhaps a simpler approach to understand how a decision tree works is to view it as a series of questions. Consider a game where your friend has chosen an activity class, and you need to guess it. You are allowed a sequence of questions, each of which could be about some characteristic about the classes. What is the minimal number of questions you might ask? In this example, the first question you might as is: Is the user in a sedentary or active state? If the answer is “active”, you might ask further questions to refine the state. The decision tree can be viewed as learning the best sequence of questions given the data and its characteristics. Decision Tree Construction using Entropy . We now understand how a decision tree works but how do we build the decision tree in the first place. The primary challenge in the construction of a decision tree is deciding on which attributes to split and in what order. One of the popular algorithms to handle this is C4.5, which uses entropy as a metric to determine the best split. Entropy . Entropy, derived from information theory, measures the level of uncertainty or randomness. In the context of decision trees, it quantifies the randomness or impurity in a label set. The for entropy for a binary classification (for simplicity) is given as: . $ \\text{Entropy}(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-) $ . Where: . | $ p_+ $ is the proportion of positive examples in $ S $ | $ p_- $ is the proportion of negative examples in $ S $ | . If the sample is completely homogeneous (either entirely positive or entirely negative), the entropy is 0. If the sample is an equally divided mixture of positive and negative examples, the entropy is 1 (maximum). Absolutely! The concept of entropy can be extended naturally to multiclass classification scenarios. Multiclass Entropy . For binary classification, the entropy formula looks at the proportions of positive and negative examples. When you have multiple classes, the formula is simply expanded to account for all these classes. Given $ C $ as the set of all classes, for a multiclass problem, entropy $ S $ is defined as: . $ \\text{Entropy}(S) = - \\sum_{c \\in C} p_c \\log_2(p_c) $ . Where: . | $ p_c $ is the proportion of samples belonging to class $ c $ in $ S $. | . For a sample that is completely homogeneous with respect to a class (i.e., all members of the sample belong to that class), the entropy is 0, just like the binary case. The maximum value of entropy depends on the number of classes. For instance, if there are 3 classes, and each class is equally represented in the sample, the entropy is $- \\frac{1}{3} \\log_2(\\frac{1}{3}) \\times 3 = \\log_2(3) \\approx 1.58$. As you can see, the more classes, the higher the maximum entropy. Information Gain . To determine which attribute should be selected as the decision node, we use a metric called Information Gain. It calculates the effectiveness of an attribute in classifying the training data. The attribute with the highest information gain is chosen as the decision node at a particular level. The formula to compute Information Gain is: . $\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v) $ . Where: . | $ S $ is the set of examples | $ A $ is an attribute | $ S_v $ is the subset of $ S $ for which attribute $ A $ has value $ v $ | . Building the Tree: C4.5 Algorithm . Pseudocode: . function BuildTree(samples, attributes) if all samples have the same label return a leaf node with that label if attributes is empty return a leaf node with the most common label of samples select the best attribute A using Information Gain create a tree T with A as the root for each value v of A add a branch to T for the test A = v let Sv be the subset of samples where A = v if Sv is empty add a leaf node with the most common label of samples else add BuildTree(Sv, attributes - {A}) to T return T . To summarize: . | For each attribute, calculate the entropy before the split and the weighted entropy of each possible split, then compute the information gain. | Choose the attribute with the highest information gain for the split, creating a node. | Divide the dataset by the values of the chosen attribute, then recursively build subtrees in the same manner. | The recursion stops once all data belongs to a single class or no attributes remain. | . This process allows for a decision tree that maximizes the reduction in randomness or impurity at each level, making it efficient in making predictions on unseen data. Note: While understanding the construction is crucial, in practical scenarios, libraries like scikit-learn offer optimized implementations of the Decision Trees that abstract away these internal details, making it easier to apply to real-world problems. Implementing Decision Trees in Python . Once we have our feature vectors prepared from raw data, we can use them to train a decision tree classifier. Decision trees are a popular algorithm because of their interpretability and ease of use. About the DataFrame . Let us assume we have a pandas DataFrame resampled_data with the following columns: x_mean, y_mean, z_mean, x_std, y_std, z_std (and potentially other feature columns) and a label column which refers to the activities you are attempting to classify. Each row in this DataFrame corresponds to a resampled window of data where: . | x_mean, y_mean, z_mean: Mean values of the X, Y, Z accelerometer readings, respectively. | x_std, y_std, z_std: Standard deviations of the X, Y, Z accelerometer readings, respectively. | label: The label or class of that particular window (e.g., ‘Sitting’, ‘Walking’). | . Implementing the Decision Tree using Scikit-learn . Scikit-learn is a popular Python library for implementing machine learning algorithms. Here’s how you can use it to implement a decision tree: . # Import necessary libraries from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier # Split the data into train and test sets features = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std'] # Add other feature columns if needed X = resampled_data[features] y = resampled_data['label'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create the decision tree classifier and train it clf = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=42) # Using Gini impurity as the criterion clf.fit(X_train, y_train) # Make predictions y_pred = clf.predict(X_test) . Parameters of the DecisionTreeClassifier . | criterion: The function to measure the quality of a split. We are using ‘entropy’ for the information gain. | max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. | random_state: Seed used by the random number generator for reproducibility. | . There are several other parameters you can set for the DecisionTreeClassifier, such as min_samples_split, min_samples_leaf, etc., which can help in regularizing the tree, preventing overfitting, or otherwise influencing the way the tree grows. After training, you can evaluate the classifier using metrics like accuracy, precision, recall, and F1-score using the true labels (y_test) and the predicted labels (y_pred). Remember to also consider regularizing your decision tree (using parameters like max_depth or min_samples_leaf) to prevent it from overfitting, especially if you have a large number of features or a small amount of training data. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree.html#table-of-contents"
  },"16": {
    "doc": "Decision Tree Classifier",
    "title": "Decision Tree Classifier",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-decision-tree.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-decision-tree.html"
  },"17": {
    "doc": "Detection vs Classification",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html#human-activity-recognition"
  },"18": {
    "doc": "Detection vs Classification",
    "title": "Table of Contents",
    "content": ". | Chapter 3: Activity Recognition using Inertial Sensors [Slides] . | Detection: Counting Steps | Classification: Recognizing Activities | From Raw Data to Activity Labels | Summing Up | . | Implementing feature extraction in Python . | The resample function | . | Notebook: Step Counting with Find Peaks [html] [ipynb] | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html#table-of-contents"
  },"19": {
    "doc": "Detection vs Classification",
    "title": "Chapter 3: Activity Recognition using Inertial Sensors [Slides]",
    "content": "Let’s turn to a concrete example to illustrate how activity recognition can work. Consider the case where we want to identify whether a person is sitting, standing, jogging, walking upstairs, walking downstairs or driving a car. Our hypothesis is that each of these activities involves a different “signature” that can help detect the current state of the user. But this process is very different from the methods that you used so far to detect steps. Detection: Counting Steps . Recall your first assignment: detecting steps. At the heart of the step counter algorithm is the detection paradigm. Here, the primary objective is to identify and quantify singular, temporally-constrained events from a continuous stream of data. In our step counting exercise, each step was represented as a peak in the accelerometer data. Using Python’s find_peaks, you designed algorithms to pick out these individual events, identifying the rhythmic up-down motion of walking. In this detection-based approach, the emphasis is on the temporal accuracy of detecting singular, repetitive events. However, this approach focuses on micro-scale details and might not be optimal for understanding broader behaviors or activities. Classification: Recognizing Activities . Now, think of activities as broader categories: walking, jogging, sitting, standing, etc. Unlike steps, these are not singular events but span over durations, often encompassing various movements and patterns. Recognizing such activities isn’t about finding a single peak but capturing the holistic ‘signature’ of a movement over time. For activity recognition, instead of analyzing each data point in isolation, we use a “rolling window” approach. This means we capture chunks (windows) of continuous data, extracting meaningful features from them, and then associating them with a specific activity label. This window can either overlap with the previous one, ensuring continuity, or be distinct, depending on our application requirements. By comparing it with the simple step detection, you can see that activity recognition deals with longer timescales. But why the rolling window? Let’s think of a classroom analogy. Instead of assessing a student based on one answer, imagine if we evaluate their performance over a series of questions. This gives a more comprehensive view of their capabilities. Similarly, by analyzing chunks of data rather than individual data points, we can understand broader patterns and behaviors. This is shown in the figure below - the table of 3-axis accelerometer data is aggregated into windows of data (100 samples in the figure). Two cases are illustrated - in the above case, each rolling window shifts by half the window size and in the below case, there is no overlap. Both are used in practice. Whether overlapping or distinct, each window represents a ‘snapshot’ of movement, which is then transformed into a set of features (we will discuss what these features are in just a bit but for now lets focus on the concept). This is a crucial distinction from the step counting, where we were only interested in specific, singular peaks. Two points to note regarding use of rolling windows: . | At a procedural level, the rolling window in classification is similar to rolling windows that we used for smoothing but with the crucial difference that we shifted by one sample in the smoothing case, whereas we shift by a full window or half window in this case. This is because the goal is different - in smoothing, we want to keep the same data length but just smooth the data whereas here, we want to aggregate data into say chunks over a longer window to determine what activity is occurring. | The window size should be selected such that it is long enough to recognize the activity of interest. For example, if the goal is to recognize walking up the stairs, then an appropriate window may be a few seconds. Neither very short windows (e.g. one second) nor very long windows (e.g. 10 minutes) would be appropriate since there would be either too little information (former case) or the activity of interest may be mixed with other activities making it difficult to recognize (latter case). | . From Raw Data to Activity Labels . With the conceptual groundwork laid, let’s understand the process of building an efficient activity recognition system. Here’s a comprehensive breakdown of each stage: . | Data Collection: The first and probably most important step of any activity recognition system is collecting high quality labeled data across many individuals, many different real-world conditions with different noise characteristics. Before diving into classification, it’s imperative to gather labeled datasets for each targeted activity. This typically involves participants or volunteers engaging in predefined activities while a recording device, often a smartphone or wearable, logs the sensor data. When generating labeled data: | . | Ensure a balanced dataset across all activity categories. | Encourage participants to carry devices in various orientations to ensure orientation-agnostic algorithms. | Gather sufficient longitudinal data per activity so you can break it down into windows. | . | Feature Extraction: The raw sensor data, while information-rich, isn’t immediately fit for classification. One needs to extract the features, the identifiable patterns, and characteristics before using the data for classification models. Here, rolling windows segment the data, with each segment undergoing feature extraction. | Time-domain Features: Simple statistical measures such as mean, median, variance, and standard deviation can capture essential characteristics of the data. | Frequency-domain Features: Techniques like Fourier Transforms convert time-domain signals into their frequency components, enabling recognition of repetitive patterns, which are crucial in activities like walking or running. | Similarity-based Features: Methods like Dynamic Time Warping (DTW) can help in understanding the similarity between two temporal sequences, useful in matching activity patterns. | Others: Depending on the application, features capturing peak values, zero-crossings, or wavelet-based characteristics might also be of interest. | . | Model Training: Having extracted a plethora of features, the next step is model training, where algorithms learn the mapping between these features and the corresponding activity labels. There are many machine learning algorithms but we will look at a few simpler ones in this class. | Decision Trees (DT): Simple yet effective, DTs can classify data based on certain decision rules. | Random Forests (RF): An ensemble of DTs, RFs offer improved accuracy and robustness. | . | Prediction: The trained model can now be downloaded to a smartphone or smartwatch for performing real-time predictions. For example, think of your smartphone or smartwatch telling you you’ve been sitting too long or congratulating you on that intense running session. These devices, equipped with inertial sensors, can leverage the classification models that you have trained to offer these insights in real-time, enhancing user experience and promoting healthy behaviors. | . Summing Up . To wrap it up, while both step counting and activity classification revolve around understanding human movement, they cater to different scales of analysis. Step counting is about detecting individual events, while activity classification seeks to recognize broader patterns over longer timescales. As we transition from counting steps to recognizing activities, we move from a micro-scale, detection-based paradigm to a macro-scale, classification-based paradigm, better suited for understanding the intricate tapestry of human behavior. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html#chapter-3-activity-recognition-using-inertial-sensors-slides",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html#chapter-3-activity-recognition-using-inertial-sensors-slides"
  },"20": {
    "doc": "Detection vs Classification",
    "title": "Implementing feature extraction in Python",
    "content": "Lets look at how you can split the data into windows of appropriate size and extract features from that window of data in python. The resample function . A python function that is particularly useful for converting raw data into a feature vector is the resample function. Lets take the case of accelerometer data with three orthogonal axes: X, Y, and Z. import pandas as pd # Assuming your data frame 'df' has a DateTime index and columns 'x', 'y', and 'z' for accelerometer readings. resampled_data = pd.DataFrame() for t, w in df.resample('100L'): frame = {} frame['time'] = t frame['x_mean'] = w['x'].mean() frame['y_mean'] = w['y'].mean() frame['z_mean'] = w['z'].mean() frame['x_std'] = w['x'].std() frame['y_std'] = w['y'].std() frame['z_std'] = w['z'].std() resampled_data = resampled_data.append(frame, ignore_index=True) . In this code: . | We resample the raw accelerometer data at 100 milliseconds intervals (‘100L’). | For each window, we compute the mean and standard deviation for the X, Y, and Z accelerometer readings. | We create a dictionary (frame) for each window, populate it with the computed features and the time stamp (t), and then append it to a new DataFrame (resampled_data). | . After executing this code, resampled_data will hold the resampled accelerometer data with features calculated for each window. Using a dictionary like this streamlines the process of iterative feature engineering and DataFrame population. The append method of pandas DataFrame can easily ingest dictionaries, where each key becomes a column in the DataFrame, and the corresponding value becomes the row entry for that column. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html#implementing-feature-extraction-in-python",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html#implementing-feature-extraction-in-python"
  },"21": {
    "doc": "Detection vs Classification",
    "title": "Notebook: Step Counting with Find Peaks [html] [ipynb]",
    "content": "This notebook shows a step counter using resample and applies it to a synthetic temperature data trace. The initial temperature signal is generated at 10Hz over 2 weeks. The notebook shows how this can be resampled into hourly and daily intervals and a few features extracted for each window. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html#notebook-step-counting-with-find-peaks-html-ipynb",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html#notebook-step-counting-with-find-peaks-html-ipynb"
  },"22": {
    "doc": "Detection vs Classification",
    "title": "Detection vs Classification",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-detection-vs-classification.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-detection-vs-classification.html"
  },"23": {
    "doc": "Freq domain features",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-freq-domain-features.html#human-activity-recognition"
  },"24": {
    "doc": "Freq domain features",
    "title": "Table of Contents",
    "content": ". | Frequency Domain Features . | Extracting Frequency Domain Features: The Role of FFT . | Dominant Frequency | Signal Energy | . | Applications and Insights | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-freq-domain-features.html#table-of-contents"
  },"25": {
    "doc": "Freq domain features",
    "title": "Frequency Domain Features",
    "content": "Frequency domain features offer insights into the periodic patterns and rhythmic behaviors inherent within a signal. Activities that manifest a repetitive or cyclic nature are especially ripe for analysis using these features. Recall the pedometer case study, where a periodic pattern in the walking signal was evident. By utilizing sophisticated techniques, we can extrapolate this rhythmic data even more effectively. Extracting Frequency Domain Features: The Role of FFT . The Fast Fourier Transform (FFT) is a pivotal algorithm that facilitates the transformation of signals from their time domain representation to their frequency domain counterpart. Essentially, while every real-world signal exists in the time domain—composed of myriad sinusoids with varying frequencies—the FFT allows us to perceive these signals through a different lens: the frequency domain. Certain characteristics and nuances of a signal, often not easily seen in the time domain, become pronounced and accessible in the frequency domain, making this important for activity classification. While a comprehensive discussion on the frequency domain features is beyond our current scope, understanding the core features and their practical applications is essential. We look at two frequency domain features in this section. | Dominant Frequency | Signal Energy | . Let’s explore these frequency domain features in detail: . Dominant Frequency . | Definition: Represents the most significant rhythmic component or periodic pattern within the data. | Utility: Provides insights into the primary repetitive behavior of an activity. | Examples: In a walking trace, the dominant frequency would typically represent the periodic footfalls. If an individual takes a step roughly every half a second, this frequency would mirror the repetitive stepping pattern. | . Signal Energy . | Definition: Measures the magnitude or intensity of variations within the accelerometer data. The more pronounced its periodic components, the higher the signal energy. | Utility: Gives an overall perspective on the robustness and amplitude of periodic elements within a signal. | Examples: Activities with clear repetitive patterns, such as cycling, would display higher signal energy due to the consistent pedal strokes. | . Applications and Insights . Visualizing a scenario of walking on a treadmill, as depicted in the subsequent figure, reveals a few dominant frequencies. Some could be harmonics of the main walking frequency. However, zeroing in on the primary dominant frequencies illuminates the core periodicity of the walking rhythm. The figure below shows an example for walking on a treadmill. You can see that there seem to be a few dominant frequencies, and some of them are harmonics of the frequency of the walking. But the dominant frequencies give you sufficient information about the periodicity of walking. Figure 1: Energy in different frequency bands for accelerometer data collected during walking . Extending our focus beyond accelerometer data, gyroscope data can also offer valuable frequency domain insights. For example, consider the context of detecting various actions in cycling such as: . | Pedal Rotations: The dominant frequency extracted from gyroscopic data during cycling would correlate to the consistent rotations of the pedals. As a rider pedals in rhythmic cycles, the gyroscope captures this behavior, making the dominant frequency a pivotal feature for analysis. | Change in Cycling Speeds: Variations in cycling speed, like transitioning from a slow pedal to a rapid sprint, will exhibit different rhythmic patterns in the data. Understanding the dominant frequency in these cases helps ascertain the main cycling rhythm. | . Thus, frequency domain features, although somewhat more complicated than their time-domain counterparts, are instrumental in discerning activities based on their rhythmic and cyclic behavior. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html#frequency-domain-features",
    
    "relUrl": "/chapter3-activityrecognition/ch3-freq-domain-features.html#frequency-domain-features"
  },"26": {
    "doc": "Freq domain features",
    "title": "Freq domain features",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-freq-domain-features.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-freq-domain-features.html"
  },"27": {
    "doc": "Overcoming Overfitting",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#human-activity-recognition"
  },"28": {
    "doc": "Overcoming Overfitting",
    "title": "Table of Contents",
    "content": ". | Introduction to Overfitting . | Why Does Overfitting Occur? | A Closer Look with Cardiac Risk Assessment | . | Addressing Overfitting: Basic Approaches . | Train-Test Split | . | The Role of Validation Sets | K-fold Cross-Validation: Beyond Simple Splits . | Understanding K-fold Cross-Validation | Advantages of K-fold Cross-Validation | Calculating Error in K-fold Cross-Validation | Implementing Train-Test Splitting in Python . | Optimizing Tree Depth to Prevent Overfitting | Other Strategies: | . | Implementing k-fold Cross-Validation in Python . | Combining Cross-Validation with Hyperparameter Tuning | . | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#table-of-contents"
  },"29": {
    "doc": "Overcoming Overfitting",
    "title": "Introduction to Overfitting",
    "content": "Overfitting is a prevalent phenomenon in machine learning where a model learns to fit the training data too closely, capturing even its noise and outliers. This means that while it performs exceptionally well on training data, its performance degrades on unseen data. Why Does Overfitting Occur? . In the context of decision trees, the depth or complexity of the tree plays a significant role in overfitting. A decision tree that is too deep will have leaves with very specific decisions, often based on individual or minimal data points. Such granularity can mean the tree is fitting to the noise in the data rather than the underlying patterns. While overfitting is particularly pronounced in decision trees due to their hierarchical nature, it’s essential to note that overfitting can occur in any machine learning algorithm. Noise in the data further exacerbates overfitting. In real-world datasets, it’s not uncommon to find inconsistencies or errors—these are referred to as noise. When a model, like a deep decision tree, learns from such data, it might treat this noise as valid patterns, leading to overfitting. A Closer Look with Cardiac Risk Assessment . Imagine our cardiac risk assessment dataset has some inconsistencies. Maybe a few healthy individuals have been mistakenly labeled as high risk due to errors in manual data entry. A deep decision tree might create specific branches to classify these individuals correctly based on the noisy data. However, when we introduce new data, these branches misclassify because they were based on incorrect data in the first place. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#introduction-to-overfitting",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#introduction-to-overfitting"
  },"30": {
    "doc": "Overcoming Overfitting",
    "title": "Addressing Overfitting: Basic Approaches",
    "content": "Cross validation is an evaluation method to identify how well the classifier will perform for data it has not already seen. One way is to not use the entire data set when training a learner. Some of the data is removed before training begins. Then when training is done, the data that was removed can be used to test the performance of the learned model on ``new’’ data. This is the basic idea for a whole class of model evaluation methods called cross validation. Train-Test Split . A foundational approach to handle overfitting by cross-validation is dividing the data into a training set and a test set. This is also referred to as the holdout method and is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. A common rule of thumb is to use 70\\% of the dataset for training and 30\\% for testing. Dividing the data into training and test subsets is usually done randomly, in order to guarantee that there is no systematic error in the process. | Train Model: Use the training set to teach the model. | Evaluate: Test its performance on the unseen test set. | Tweak and Repeat: Based on the test set performance, make changes to the model, train it again, and keep repeating this loop. | . By evaluating the model on the test set, which it hasn’t seen during training, we get a clearer picture of how the model performs on new, unseen data. The idea is to select the model version that gives the best performance on the test set. However, its evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#addressing-overfitting-basic-approaches",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#addressing-overfitting-basic-approaches"
  },"31": {
    "doc": "Overcoming Overfitting",
    "title": "The Role of Validation Sets",
    "content": "While the train-test methodology helps, there’s still room for over-optimizing on the test set, especially when we make numerous tweaks based on test set performance. To avoid this, we introduce a third dataset called the validation set. | Train the model on the training set. | Tweak and tune the model based on performance on the validation set. | Only once you’ve finalized the model, evaluate it on the test set to get an unbiased performance metric. | . Having a separate validation set allows us to make adjustments without biasing our model to the test data. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#the-role-of-validation-sets",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#the-role-of-validation-sets"
  },"32": {
    "doc": "Overcoming Overfitting",
    "title": "K-fold Cross-Validation: Beyond Simple Splits",
    "content": "While train-test and train-validation-test splits are fundamental to avoiding overfitting, they might not always be enough. This is where k-fold cross-validation comes into play. Understanding K-fold Cross-Validation . Instead of dividing the dataset into two or three static sets, k-fold cross-validation divides the dataset into ( k ) different subsets (or “folds”). The model is trained and evaluated ( k ) times, each time using a different fold as the test set and the remaining ( k-1 ) folds combined as the training set. This process is iterated until every fold has been used as the test set. For example, in a 5-fold cross-validation: . | In the first iteration, Fold 1 is the test set, and Folds 2-5 combined are the training set. | In the second iteration, Fold 2 becomes the test set, while Folds 1, 3, 4, and 5 are the training set. … and so on until the fifth iteration. | . Advantages of K-fold Cross-Validation . | Better Utilization of Data: Especially in scenarios where the dataset is limited in size, k-fold cross-validation ensures that every data point has been in the test set exactly once and in the training set ( k-1 ) times. This comprehensive utilization can lead to a more reliable estimate of model performance. | Reduced Variability: By averaging the results from the ( k ) folds, we can reduce the variance associated with a single random train-test split. This leads to a more stable and generalized model assessment. | Mitigates Overfitting: With multiple train-test splits, the model’s ability to generalize is tested more rigorously, making it less likely that the model is overfitting to a specific subset of the data. | . Calculating Error in K-fold Cross-Validation . The final performance metric in k-fold cross-validation is typically the average of the errors across the ( k ) iterations. If ( E_i ) represents the error in the ( i^{th} ) fold, the overall error ( E ) is calculated as: . [ E = \\frac{1}{k} \\sum_{i=1}^{k} E_i ] . By taking the average error across the folds, we ensure that the model’s performance is not overly influenced by any single partition of the data. Overfitting remains one of the primary challenges in machine learning. Recognizing its signs and employing techniques like train-test splits and validation sets are crucial first steps in combating it. K-fold cross-validation, with its systematic approach to training and evaluating, offers a robust mechanism to understand a model’s performance. While it can be computationally more intensive, the insights gained from multiple evaluations often outweigh the costs, leading to more trustworthy and generalizable models. Implementing Train-Test Splitting in Python . from sklearn.model_selection import train_test_split features = ['x_mean', 'y_mean', 'z_mean', 'x_std', 'y_std', 'z_std'] # and other feature columns if needed X = resampled_data[features] y = resampled_data['label'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . The test_size parameter defines the proportion of the dataset to include in the test split. A common split ratio is 80\\% training and 20\\% testing. Optimizing Tree Depth to Prevent Overfitting . A simple way to prevent overfitting in decision trees is to control the depth of the tree. A tree that is too deep might overfit the training data, while a very shallow tree might underfit. To find the optimal depth, you can: . | Train multiple trees with varying depths. | Evaluate each tree on the test set. | Choose the depth that gives the best performance on the test set. | . Here’s a simple example using scikit-learn: . from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score best_depth = 1 best_accuracy = 0 # Check for depths from 1 to 10 for depth in range(1, 11): clf = DecisionTreeClassifier(max_depth=depth, random_state=42) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) acc = accuracy_score(y_test, y_pred) if acc &gt; best_accuracy: best_accuracy = acc best_depth = depth print(f\"Best depth is: {best_depth} with accuracy: {best_accuracy}\") . Other Strategies: . | Pruning: Instead of letting the tree grow to its maximum depth, you can prune it to remove nodes that add little power to the classification. | Minimum Split Size: Set the minimum number of samples required to split an internal node. | Minimum Leaf Size: Set the minimum number of samples required to be at a leaf node. | Cross-Validation: Instead of a single train/test split, use k-fold cross-validation to get an average performance metric, which will provide a more robust performance estimate. | Feature Importance: Some features might be noisy. Decision trees in scikit-learn provide a feature_importances_ attribute, which can be used to understand which features are most informative and potentially remove or adjust features that add noise. | . Remember, while it’s important to ensure your model doesn’t overfit the training data, it’s equally crucial to ensure it doesn’t underfit by being too simplistic. Balancing complexity and simplicity is key to building a robust decision tree classifier. Implementing k-fold Cross-Validation in Python . Here’s how you can implement k-fold cross-validation using scikit-learn: . from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier # Define the classifier clf = DecisionTreeClassifier(max_depth=3, random_state=42) # Example depth # Use cross_val_score for k-fold cross-validation k = 5 # for a 5-fold cross-validation scores = cross_val_score(clf, X, y, cv=k, scoring='accuracy') average_score = scores.mean() print(f\"Average accuracy across {k} folds: {average_score:.2f}\") . The advantage of k-fold cross-validation is that it uses the entire dataset for both training and validation, ensuring a more comprehensive evaluation. However, keep in mind that it requires training the model (k) times, which might be computationally expensive for larger datasets or complex models. Combining Cross-Validation with Hyperparameter Tuning . When optimizing hyperparameters, such as the depth of a decision tree, it’s beneficial to combine k-fold cross-validation. By doing this, you ensure that the selected hyperparameters are robust across different data subsets. Here’s an example using GridSearchCV which will search for the best depth while performing k-fold cross-validation: . from sklearn.model_selection import GridSearchCV # Define the parameter grid param_grid = {'max_depth': range(1, 11)} # Create the grid search object with k-fold cross-validation grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=k, scoring='accuracy') # Fit the grid search object to the data grid_search.fit(X, y) # Get the best parameters and score best_depth = grid_search.best_params_['max_depth'] best_score = grid_search.best_score_ print(f\"Best depth found using {k}-fold cross-validation: {best_depth}\") print(f\"Best average accuracy: {best_score:.2f}\") . This combined approach will give a more robust estimation of the model’s performance and the optimal hyperparameters, reducing the risk of overfitting on the training data. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html#k-fold-cross-validation-beyond-simple-splits",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html#k-fold-cross-validation-beyond-simple-splits"
  },"33": {
    "doc": "Overcoming Overfitting",
    "title": "Overcoming Overfitting",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-overfitting.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-overfitting.html"
  },"34": {
    "doc": "Time domain features",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-time-domain-features.html#human-activity-recognition"
  },"35": {
    "doc": "Time domain features",
    "title": "Table of Contents",
    "content": ". | Time Domain Features . | Key Features: | Mean | Median | Variance &amp; Standard Deviation | Min, Max, &amp; Range | Zero-Crossings | Other Features | Features from Gyroscopes and Magnetometers . | Gyroscope | Magnetometer | Combining Features | . | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-time-domain-features.html#table-of-contents"
  },"36": {
    "doc": "Time domain features",
    "title": "Time Domain Features",
    "content": "Time-domain features represent statistical measures derived from a sequence of samples within a specific window, such as 100 samples. These features serve as essential building blocks for discerning patterns and categorizing different activities based on accelerometer data. Key Features: . | Mean | Median | Variance | Standard deviation | Min | Max | Range | Zero-crossings | Angle | Angular velocity | . Let’s delve deeper into some of these accelerometer-based time-domain features: . Mean . | Definition: The average of acceleration values over a window. | Utility: Helps in differentiating activities based on their overall acceleration magnitude. | Examples: Clearly distinguishes between standing (which typically has a near-zero mean) and more dynamic activities like walking or running which exhibit higher means. | Confounders: Might blur the distinction between sitting and standing since both can portray similar mean values. | . Median . | Definition: The middle value once acceleration values are arranged in order. | Utility: Provides a more robust measure in the presence of outliers compared to the mean. | Examples: Helps differentiate between regular walking and sudden jolts during walking which might be seen as outliers. | . Variance &amp; Standard Deviation . | Definition: Variance indicates the dispersion of numbers from the mean. The standard deviation is essentially the square root of variance. | Utility: Enables differentiation of activities based on variability. | Examples: Running, for instance, might exhibit a higher variance than walking. Additionally, walking upstairs can reflect a distinct variance pattern in contrast to flat surface walking. | Confounders: There might be overlapping variance values for activities that are somewhat similar. | . Min, Max, &amp; Range . | Definition: Represents the minimum and maximum acceleration values within a window. The range is the differential between these values. | Utility: Recognizes the extreme limits of motion. | Examples: Activities like jumping can display a higher range than walking due to the drastic up-and-down motions involved. | . Zero-Crossings . | Definition: Counts the instances when the signal transits through a zero value. | Utility: Particularly beneficial for repetitive or cyclical motions. | Examples: Activities such as walking or running may manifest periodic zero-crossings, attributed to the consistent foot-strike patterns. | Confounders: Excessive noise within data can give rise to a higher count of zero-crossings. | . Other Features . Cross-Correlation - Measures the similarity between two signals relative to the delay imposed on one of them. For instance, comparing acceleration in the X-axis with that in the Y-axis. Number of Peaks and Troughs - Presents data on the repetitiveness and intensity of specific activities. Activities with a higher frequency like jogging will likely register more peaks within a specific window than walking. Slope of Zero Crossing - Indicates the rate of value change during a zero crossing. This can be pivotal in distinguishing between slow-paced and rapid movements. Features from Gyroscopes and Magnetometers . Although the primary emphasis has been on accelerometers, other sensors like gyroscopes and magnetometers are equally valuable. Gyroscopes record rotational movements, while magnetometers detect magnetic field intensity. Gyroscope . Angle &amp; Angular Velocity: Provides insights into the rotation speed of an object. Activities involving turning or spinning will generally indicate higher values. | Definition: Angle denotes orientation inferred from accelerometer and gyroscope data integration. Angular velocity measures the change rate of this angle. | Utility: Offers insights into orientation and its variations. | Examples: Different body postures like standing and sitting might reflect varying orientations. Movements such as turning or twisting can be effectively captured using angular velocity. | . Magnetometer . | Magnetic Field Strength: Presents orientation data in relation to the Earth’s magnetic field. | Utility: Can assist in determining the direction an individual is moving or facing. | . Combining Features . Leveraging a single feature may not always be sufficient in differentiating activities, especially when there’s an overlap. However, amalgamating multiple features can create a multi-dimensional realm, making activity categorization more precise and definitive. For instance, using just the mean and variance might not clearly differentiate between ascending and descending stairs, but their combination could provide a sharper distinction. ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html#time-domain-features",
    
    "relUrl": "/chapter3-activityrecognition/ch3-time-domain-features.html#time-domain-features"
  },"37": {
    "doc": "Time domain features",
    "title": "Time domain features",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-time-domain-features.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-time-domain-features.html"
  },"38": {
    "doc": "Visualizing activities",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/ch3-visualizing-activities.html#human-activity-recognition"
  },"39": {
    "doc": "Visualizing activities",
    "title": "Table of Contents",
    "content": ". | Visualizing Common Activities . | References | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/ch3-visualizing-activities.html#table-of-contents"
  },"40": {
    "doc": "Visualizing activities",
    "title": "Visualizing Common Activities",
    "content": "Let us consider six activities: walking, jogging, ascending stairs, descending stairs, sitting, and standing. We selected these activities because they are performed regularly by many people in their daily routines. The activities also involve motions that often occur for substantial time periods, thus making them easier to recognize. Furthermore, most of these activities involve repetitive motions and hopefully this should also make the activities easier to recognize. Directional sensitivity of accelerometer. To aid in recognition, it’s crucial to understand the directional sensitivity of accelerometers: . | X-Axis: Captures horizontal movement of the user’s leg. | Y-Axis: Reflects upward and downward motion. | Z-Axis: Represents the forward movement of the leg. | . It’s worth noting that the Y-values typically have the most significant accelerations across activities. This prevalence is attributed to Earth’s gravitational pull, which leads to a consistent 9.8 m/s^2 acceleration in the direction of the Earth’s center. For all activities, barring sitting, this direction predominantly aligns with the Y-axis. Figure 1: Accelerometer data for sitting and standing (viz. from [1]) . Sitting and Standing. It is clear that sitting and standing do not exhibit periodic behavior but do have distinctive patterns, based on the relative magnitudes of the x, y, and z, values. The primary difference between sitting and standing is the relative magnitudes of values for each axis, due to the different orientations of the device with respect to the Earth when the user is sitting and standing. Thus it appears easy to differentiate between sitting and standing, even though neither involves much movement. Figure 2: Accelerometer data for walking, jogging, ascending/descending stairs (viz. from [1]) . Walking, jogging, ascending/descending stairs. The periodic patterns for walking, jogging, ascending stairs, and descending stairs can be described in terms of the time between peaks and by the relative magnitudes of the acceleration values. The plot for walking demonstrates a series of high peaks for the y-axis, spaced out at approximately ½ second intervals. The peaks for the z-axis acceleration data echo these peaks but with a lower magnitude. The distance between the peaks of the z-axis and y-axis data represent the time of one stride. The x-axis values (side to side) have an even lower magnitude but nonetheless mimic the peaks associated with the other axes. For jogging, similar trends are seen for the z-axis and y-axis data, but the time between peaks is less (~¼ second), as one would expect. The range of y-axis acceleration values for jogging is greater than for walking, although the shift is more noticeable in the negative direction. For descending stairs, one observes a series of small peaks for y axis acceleration that take place every ~½ second. Each small peak represents movement down a single stair. The z-axis values show a similar trend with negative acceleration, reflecting the regular movement down each stair. The x-axis data shows a series of semi-regular small peaks, with acceleration vacillating again between positive and negative values. For ascending stairs, there are a series of regular peaks for the z-axis data and y-axis data as well; these are spaced approximately ~¾ seconds apart, reflecting the longer time it takes to climb up stairs. Having looked at the data for the activities that we want to distinguish, let’s turn our attention back to designing the classifier that will automatically determine user state. References . | Feature Engineering on Time-Series Data for Human Activity Recognition, Pratik Nabriya | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html#visualizing-common-activities",
    
    "relUrl": "/chapter3-activityrecognition/ch3-visualizing-activities.html#visualizing-common-activities"
  },"41": {
    "doc": "Visualizing activities",
    "title": "Visualizing activities",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/ch3-visualizing-activities.html",
    
    "relUrl": "/chapter3-activityrecognition/ch3-visualizing-activities.html"
  },"42": {
    "doc": "PPG Analysis",
    "title": "PPG Analysis",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Analysis of ECG and PPG waveforms. In this note, we will look briefly at how to extract key metrics like heart rate and breathing rate from ECG and PPG waveforms. ### Electrocardiogram (ECG) ECG (or EKG) is a recording of the electrical activity of the heart. With each heartbeat, an electrical signal spreads from the top of the heart to the bottom. As it travels, the signal causes the heart to contract and pump blood. The process repeats with each new heartbeat. The heart's electrical signals set the rhythm of the heartbeat. This can be measured by placing two electrodes at different points on the chest, and measuring the electrical activity between these electrodes. An ECG shows how fast your heart is beating, the rhythm of your heartbeat (steady vs irregular), and the strength and timing of the electrical signals as they pass through each part of the heart. Lets take a look at a typical ECG signal, and what types of features are useful to extract from the ECG signal. A typical ECG wave looks as shown in the figure above. Each heartbeat comprises a sequence of peaks and trough, labeled P, Q, R, S, and T as shown in the figure. Occasionally, there is a U wave after T as well. This pattern repeats as long as a person’s heart is beating. ### How do you detect peaks and troughs in an ECG waveform? You might be thinking that these peaks and troughs look quite similar to the step detector that we discussed in Lesson 2. Indeed, the peak detection algorithm that we used for the step detector is indeed a good starting point for detecting the peaks and troughs in ECG. As with step detection, you look for a change in the slope from positive to negative (peak) or negative to positive (trough). You then look at the sequence information to label the appropriate peaks (P, Q, R, S, T). ### How to extract ECG features? Once you have the five (or six) peaks and troughs, the timing differences between them are useful features for classification. For example, you can extract the RR interval, PR interval, the QRS interval, the QT interval, the ST interval, and so on, and look at how these intervals vary to detect abnormalities in the heart. Obtaining heart rate from ECG is quite straightforward once you have the RR interval. Each RR interval corresponds to the time between two successive heartbeats, so you just need to compute the number of RR intervals in one minute, which is _HR = 60 &‌divide; RR_ (when RR is measured in seconds). You can also measure the breathing rate from heart rate information, photoplethysmography (PPG) in the next section. ## Photoplethysmography (PPG) Photoplethysmography is a non-invasive technique for measuring blood volume changes in the blood vessels close to the skin. PPG has become a popular non-invasive method for extracting physiological measurements such as heart rate and oxygen saturation. PPG is typically measured by a device called a pulse oximeter, that is worn like a cap on the index finger (on the left below). But what makes PPG particularly appealing is that it can be measured using a built-in cellphone camera without any additional hardware. Well, almost no additional hardware, since its often useful to have a cap as shown on the right below to block out ambient light. By placing the index finger over the cellphone camera with its flash turned on, the camera records the light absorbed by the finger tissue. Then, from the video, each frame is processed by splitting every pixel into red, green and blue (RGB) components, which are then processed to extract heart rate and breathing rate. There are many apps that are available on the Android Store and iOS App Store that use photoplethysmography --- some examples are _[Whats my Heart Rate](https://play.google.com/store/apps/details?id=com.vitrox.facion.gui&hl=en)_, which is an Android App, and _[Instant Heart Rate ](http://itunes.apple.com/us/app/instant-heart-rate-by-azumio/id395042892)_and _[Cardiograph](http://itunes.apple.com/us/app/cardiograph/id441079429)_, which are iPhone apps. Lets take a look at how these apps work. ### How to extract Heart Rate (HR) from PPG? During the cardiac cycle, when the heartbeats, it creates a wave of blood that reaches the capillarity at the tip of the finger, when the capillarity is full of blood, less light passes through and the finger. When the blood retracts, more light can pass through the tissue. If these changes are recorded over time, a waveform is going to be created that correspond to the pulsatile changes in the arterial blood in that tissue. These changes in the arterial blood volume correspond to the heart rate. This process is known as PPG. Using the method described above we try to detect the cardiac waveform and from that the heart rate. Heart rate estimation from the signal is as follows. From the recorded video, the green values from every frame were extracted in order to acquire the PPG signal. The green intensity average in the PPG signal formed peaks that correspond to cardiac pulse. A peak detection algorithm was used in order to find all the cardiac peaks in the signal. A peak is defined as the highest average of green values in a fixed window size (typically ~0.7 seconds). Once a peak was found, the time difference between consecutive peaks was computed. This time difference is known as R-R interval (RRI). From the R-R interval values the HR was estimated as _HR = 60 &‌divide; RR_ (when RR is measured in seconds). ### How to extract Breathing Rate (BR) from PPG? Interestingly, the PPG signal can be used for extracting not just the heart rate but also the breathing rate of the individual. The reason why this is possible is because of a phenomenon called Respiratory sinus arrhythmia (RSA), which is a naturally occurring variation in heart rate that occurs during a breathing cycle. Heart rate increases during inspiration and decreases during expiration! Were you aware of this? We will skip the biological reasons for RSA, and focus on how to extract breathing rate from PPG. To view this behavior in a real dataset, focus on the blue curve in the figure below (ignore the red curve for now). The blue curve plots the average green intensity per frame. As we expected, the green intensity goes up and down for each heartbeat. But you can also see that there is another periodic pattern that is visible --- the green intensity for each heartbeat is not identical, in fact the green intensity seems seems to spike every few heartbeats and go down after that. This pattern is caused by RSA as described earlier -- the R-R interval on an ECG is shortened during inhalation and prolonged during exhalation. How do we go about extracting the respiration rate (RR)? One method is to look for the frequency of changes in the heart rate (which in turn corresponds to the breathing rate). Whenever we want frequency domain information, we use an FFT to convert from time to frequency domain, and take the dominant frequency from the FFT. This is exactly what we do to obtain the respiration rate from the PPG signal. Note that the breathing waveform can be noisy and somewhat difficult to extract precisely from PPG. So, an FFT may not always work as expected. That completes our analysis of how PPG can be used to extract heart rate and breathing rate. To summarize, a) first, we get the average green intensity per frame, b) second, we use peak detection on a short window of samples to extract the RR interval, and from that obtain the HR, and c) third, we perform an FFT on the RR intervals obtained in step (b) to obtain the RR. The only thing left for you to do is to try this out on your smartphone! ## References [Extracting Heart Rate and Respiration Rate using a Cell Phone Camera ](https://parasol.tamu.edu/dreu2013/Jimenez/documents/EXTRACTING%20HEART%20RATE%20AND%20RESPIRATION%20RATE%20USING%20A%20CELL%20PHONE%20CAMERA.pdf)- Jimenez, Parnandi, Gutierrez-Osuna ",
    "url": "/mhealth-course/chapter4-heartrhythm/ch4-ecg-ppg-analysis.html",
    
    "relUrl": "/chapter4-heartrhythm/ch4-ecg-ppg-analysis.html"
  },"43": {
    "doc": "Photoplethysmography (PPG)",
    "title": "Photoplethysmography (PPG)",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Measuring Heart Rhythm via Photoplethysmography (PPG) _This note is adapted from Valencell's excellent post on [Optical Heart Rate Monitors](https://valencell.com/blog/optical-heart-rate-monitoring-what-you-need-to-know/)._ Optical heart rate monitors are used in a variety of wearable devices today including wrist bands, smartwatches, audio earbuds, pulse oximeters, hearing aids and many more. These monitors are driving a wide variety of use cases and applications today. We typically see three primary scenarios: 1\\. Lifestyle – Typically this involves tracking variables like steps, basic movement, resting and/or casual heart rate, etc. Comfort and style are typically valued over accuracy in this scenario, although this is changing with rising consumer interest in health and fitness analysis. 2\\. In-session – Focused on real-time biometric measurement during a specific activity such as working out, running, biking or even fighting a fire. Stability and accuracy are highly valued in these scenarios. 3\\. Personal Health – Ongoing measurement of personal health indicators such as heart rate, blood pressure, oxygen saturation, etc. These measurements can be used in conjunction with a prevention plan (in healthy populations) or a disease management plan (for those managing a health condition such as hypertension, diabetes, cardiovascular disease, etc.).  Accuracy and comfort are highly valued in these scenarios. _Figure 1: Common use cases of PPG monitors._ ## How does Optical Heart Rate Monitoring work? Most wearables with heart rate monitors today use a method called [photoplethysmography (PPG)](https://en.wikipedia.org/wiki/Photoplethysmogram) to measure heart rate. PPG is a technical term for shining light into the skin and measuring the amount of light that is scattered by blood flow. That’s an oversimplification, but PPG sensors are based on the fact that light entering the body will scatter in a predictable manner as the blood flow dynamics change, such as with changes in blood pulse rates (heart rate) or with changes in blood volume (cardiac output). PPG sensors use four primary technical components to measure heart rate: 1\\. Optical emitters – generally at least 2 LED’s that send light waves into the skin, although some PPG sensors are adding more emitters and varying light wavelengths. Because of the wide differences in skin thickness, tone and morphology associated with a diversity of people, most state-of-the-art optical heart rate monitors use multiple light wavelengths that interact differently with different levels of skin and tissue. 2\\. Photodetector(s) – the photodetector captures the light refracted from the user of the device and translates those signals into one’s and zero’s that can be calculated into meaningful heart rate data. 3\\. Accelerometer – the accelerometer measures motion and is used in combination with the photodetector signal as inputs into PPG algorithms. 4\\. Algorithms – the algorithms process the signals from the photodetector and the accelerometer into motion-tolerant heart rate data, but can also calculate additional biometrics such as calories burned, R-R interval, heart rate variability, and blood oxygen levels. _Figure 2: MEMS Motion Sensor - Optical Sensors_ ## History of PPG PPG is actually almost 150 years old, but it has been revolutionized in the 21st century for new use cases. Real-time optical blood flow monitoring was first used in the late 1800s by having people hold their hand up to a candle in a dark room to see the vascular structure and blood flow. More recently in the early 1980s, the first pulse oximeters were launched for hospital use, measuring pulse rate and blood oxygen using two alternating LEDs. These are very similar to the finger or ear clip devices still used in healthcare facilities today. PPG sensor developments in the last 5-10 years have focused on consumer and medical wearable devices and services. This required a radical development known as motion-tolerant PPG because using PPG sensors during motion and activity massively increases the amount of motion noise that must be removed to find the blood flow signal. Here’s a brief visual history of PPG sensors: _Figure 3: Optical heart rate monitoring - technology timeline._ ## What are the primary challenges with optical heart rate monitors? PPG sounds relatively simple, but it’s actually very difficult to implement accurately for wearables. Measuring PPG during a resting state (sleeping, sitting, and standing still) is relatively straightforward, but measuring PPG during physical activity is incredibly complex. There are five fundamental challenges that one faces in analyzing PPG signals: 1\\. _**Optical noise**_ – The biggest technical hurdle in processing PPG sensor signals is separating the biometric signal from the noise, especially motion noise. Unfortunately, when you shine light into a person’s skin only a small fraction of the light returns to the sensor, and of the total light collected, only ~1/1000th of it may actually indicate heart-pumped blood flow. The rest of the signals are simply scattered by other material, such as skin, muscle, tendons, etc. 2\\. _**Skin tone**_ – Humans have a diverse range of skin tones and different skin tones absorb light differently. For example, darker skin absorbs more light, which presents a problem because many optical heart rate monitors don’t use the right mitigations to accurately measure heart rate through dark skin. This also presents a problem for measuring heart rate through tattooed skin, which Apple found out the hard way in what became known as “tattoogate” when people with wrist tattoos found that the heart rate monitor on the Apple Watch performed poorly – or not at all – for them. 3\\. _**Crossover problem**_ – One of the most challenging aspects of optical noise for optical heart rate monitors that is created by motion and activity happens during what is known as periodic activity, which is an activity that involves continuous repetition of similar motion. This is most often seen in the step rates measured during jogging and running because step rates typically fall into the same general range as that of heartbeats (140-180 beats/steps per minute). The problem that many optical monitors face is that it becomes easy for the algorithms interpreting incoming optical sensor data to mistake step rate (“cadence”) for heart rate. This is known as the **crossover problem**, because if you look at the measurements on a graph when the heart rate and step rate crossover each other, many optical heart rate monitors tend to lock on to step rate and present that number as the heart rate, even though the heart rate may be changing drastically after the crossover. 4\\. _**Sensor location**_ – Heart rate monitoring presents unique challenges that vary significantly by location. It turns out that the wrist is a poor location for accurate PPG monitoring of heart rate because of the much higher optical noise created in that region (muscle, tendon, bone, etc.) and because of the high degree of variability in vascular structure and blood [perfusion](https://en.wikipedia.org/wiki/Perfusion) across the human populations. The forearm is considerably better because of the higher density of blood vessels near the surface of the skin. The ear is a great location on the body for PPG monitoring because it is essentially just cartilage and blood vessels, which don’t move much even when the body is in vigorous motion, thereby drastically reducing the optical noise that must be filtered. 5\\. _**Low perfusion**_ – Perfusion is the process of a body delivering blood to capillary beds. As with skin tone, the level of perfusion is highly variable across populations, with issues such as obesity, diabetes, heart conditions, and arterial diseases each lowering blood perfusion. Low perfusion, especially in the body’s extremities where most wearable devices are located, can present challenges for optical heart rate monitors because the signal-to-noise ratio may be drastically reduced, as lower perfusion correlates with lower blood flow signals. The head region (including the ear, temple, and forehead) supports much higher perfusion and better quality photoplethysmograms than the wrists or feet. ## How do we improve PPG monitoring quality? PPG is obviously very difficult to measure accurately, particularly from the wrist but wearable devices are getting better at it. At a high level, this requires advances in optomechanics (hardware design) and signal processing/machine learning algorithms (software). Here are a few ways of improving the signal analysis algorithm. 1. Have the algorithms been validated on a diverse population set? It’s important to make sure the device works on different skin tones, genders, health conditions, body types and levels of fitness, etc. 2. Are the algorithms robust to multiple types of motion noise? The algorithms must be able to work during different activities, including walking, running (high-speed steady runs and interval training), sprinting, gym activities (weight lifting, Crossfit, etc.) and everyday life activities like typing on a computer, talking on the phone, or riding in a vehicle. 3. Is the signal extraction methodology scalable to multiple form factors i.e. devices? You don’t want to have to use different algorithms for each different form factor you may want to use. ## What metrics can you get from PPG sensors? While PPG is really hard to get right, when you do get it right, it can be very powerful. A high-quality PPG signal is foundational to a wealth of biometrics that the marketplace is demanding today. Here is a a list of the most common metrics that we can obtain from PPG. * [Breathing rate](https://en.wikipedia.org/wiki/Respiratory_rate) – breathing rate is the number of breaths taken in a period of time (typically 60 seconds) and lower resting breathing rates are generally correlated with higher levels of fitness. * [Blood oxygen levels (SpO2, oxygen saturation)](https://en.wikipedia.org/wiki/Oxygen_saturation_(medicine)) – blood oxygen levels indicate the concentration of oxygen in the blood. The latest Apple Watches use this technology. * [Heart rate variability](https://en.wikipedia.org/wiki/Heart_rate_variability) – Heart rate variability captures the variation in the interval between blood pulses (or ECG beats), and generally the more varied the time between beats, the better. Heart rate variability analysis can be used as an indicator of stress levels and various cardiac issues, among other things. * Cardiac efficiency – this is another indicator of fitness that typically measures how efficiently your heart works to take one step. This serves as a proxy for how hard your heart would have to work to do more challenging exercises like running or cycling. You can see below a simplified PPG signal and where each of the biometrics is measured within that signal. _Figure 6: PPG metrics and how they are extracted._ ",
    "url": "/mhealth-course/chapter4-heartrhythm/ch4-ppg.html",
    
    "relUrl": "/chapter4-heartrhythm/ch4-ppg.html"
  },"44": {
    "doc": "Audio Classification",
    "title": "Audio Classification",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Classification of Health State Voice is an extremely useful and important part of health analytics. There are many interesting questions that can be answered with audio processing -- for example, how much social interaction does a person engage in? what is the emotional content of the voice? and so on. These indicators can be leveraged for understanding depression, stress, mood, and many other factors that are relevant to personal health. We will now look at some interesting ways in which voice processing can be used for obtaining measures of health. ### Voice Analysis Library: Classification We’re now going to look at how the different features listed above can be used in a variety of classification tasks. Audio classification is a vast field, and a detailed description of the technique will require both substantial time and significant mathematical maturity. So, we stay at a more superficial level, and try to provide you an idea of the techniques so that you can explore on your own if you are interested. We start with speech processing (e.g. Siri), and then discuss one example of an emotion recognition system. #### Speech processing Speech processing is a field that has seen a huge amount of work. There are complex models, and complex algorithms underlying them that are out of scope for this class. Our goal here is only to introduce the ideas so that you are familiar with how a basic speech processing system can be constructed. Human speech can be broken into phonemes -- for example, when you look up a word in a dictionary, you get the phonetic pronunciation of the word, which is a combination of phonemes. An example of a phoneme in English is /k/, which occurs in words such as **_cat_**, **_kit_**, _s**ch**ool_, _s**k**ill_, **_kite._** The challenge in speech recognition is to recognize a sequence of phonemes as a particular word. In the example above, lets say a word starts with the phoneme /k/ (assume for the moment that we can identify start of words). There are a huge number of words that start with the phoneme /k/, so how does the system identify which is the word being spoken? To do this, speech recognizers use temporal models such as a Hidden Markov Model (HMM). At a high level, a HMM for each letter in the alphabet tries to look for a sequence of phonemes, while taking into account the fact that different people pronounce letters differently (therein the distribution shown below each circle in the figure below). Hidden Markov Models are extremely powerful, and can be composed to identify letters, then words, and then sentences as shown below. As described earlier, the most useful feature for speech recognition is the acoustic feature vector (MFCC + others). These are the core features that are used to distinguish between different phonemes, and thereby the different letters, words, sentences, etc. #### Diagnosis of Mental Illnesses Mental illness such as depression are well known to have substantial effect on voice. Table 1 shows the aspects of speech that mental health practitioners pay particular attention to when performing a mental state examination of patients. These parameters help describe a patient's current state of mind, under the domains of appearance, attitude, behavior, mood and affect, speech, thought process, thought content, perception, cognition, insight and judgment. For example, depressed patients often express slow responses (longer response time to questions and pause time within sentences), monotonic phrases (less fundamental frequency variability), and poor articulation (slower rate of diphthong production). On the other hand, the spectrum of agitated behavior includes expansive gesturing, pacing and hair twirling. As can be seen, many of the prosodic features capture these variations; in addition, speech related features such as MFCC capture speech intensity. | Category | Patterns | . | Rate of speech | slow, rapid | . | Flow of speech | hesitant, long pauses, stuttering | . | Intensity of speech | loud, soft | . | Clarity | clear, slurred | . | Liveliness | pressured, monotonous, explosive | . | Quality | verbose, scant | . _Table 1: Speech Descriptors in Mental Status Exam_ ### Monitoring Affect with a Mobile Phone There are many interesting applications for affect/mental health monitoring by voice. Consider the following examples: **Cell-phone monitoring of healthy subjects as part of a health-care package.** Using voice analysis algorithms on the phone itself, the voice is analyzed during cell phone conversations. Subjects are directly informed if problems are detected by the voice analysis software. In the early stages, they are likely to seek treatment in many cases. Rather than individual sign-up, this would be part of a health “package\" from a provider which includes physical health as well. **Subsidized “calling-card\" number for at-risk populations.** Subjects can make free calls on a normal phone using a special access number. This number routes calls through a cloud of servers where voice is analyzed. Distinct access codes would allow per-patient tracking. This method should be cost effective for many chronic conditions such as AIDS where mentally ill patients add severe cost overhead due to wasted (not taken) medication and (corollary) drug-resistance strains of the virus. **Monitoring of human-computer speech interfaces and interpersonal speech for elders in assisted or independent care.** Environmental monitors (array microphones that work 10-20 feet from subjects) can be used to gather incidental speech between subjects. For subjects living alone, speech interfaces may be introduced as a convenient way for subjects to access email, text messages, news, weather and personal schedule information. These routine services provide regular opportunities to intercept and analyze their speech for mental health purposes. **Monitoring our stress in everyday lives.** Microphones, embedded in mobile phones and carried ubiquitously by people, provide the opportunity to continuously and non-invasively monitor stress in real-life situations using an approach very similar to that for identifying depression. Everyone experiences stressful situations, either because of work pressures, exams, personal circumstances, or others. Maybe your phone can inform you that you should do something about your stress levels, perhaps play calming music, or take a break. **Monitoring Social Interactions (or lack of it).** Your phone can recognize when there is conversation in the vicinity (using speech-based features), and also whether you are speaking or someone else (since individuals have distinctive speech patterns). Using this information, your phone may be able to monitor your daily social interactions, and identify whether you have gone long periods without contact with someone (a common problem in the digital age). Maybe it can give you notifications that its time to get off your computer and socialize! ### Conclusion Speech and sound is extremely important in our lives, and an amazing amount of information can be gleaned from recorded audio from a simple microphone. As microphones become more powerful and more ubiquitous, the possibility that computing systems will be able to monitor 24/7 and identify changes in health patterns has exciting possibilities for understanding ourselves and in anticipating problems. But it also presents many roadblocks, not just in the audio data analysis, but also in making sure people’s privacy is not violated in the process. ### References [1] Speech Emotion Classiﬁcation using Machine Learning Algorithms, S. Casale, A. Russo, G. Scebba [2] [Speech Analysis Methodologies towards Unobtrusive Mental Health Monitoring](http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-55.pdf), Keng-hao Chang [3] OpenEAR - Introducing the Munich Open-Source Emotion and Affect Recognition Toolkit, Florian Eyben, Martin Wollmer, and Bj ¨ orn Schuller [4] StressSense: Detecting stress in unconstrained acoustic environments using smartphones, Lu, H., Frauendorfer, D., Rabbi, M., Mast, M. S., Chittaranjan, G. T., Campbell, A. T., ... & Choudhury, T. ",
    "url": "/mhealth-course/chapter5-voiceanalytics/ch5-audioclassification.html",
    
    "relUrl": "/chapter5-voiceanalytics/ch5-audioclassification.html"
  },"45": {
    "doc": "Audio Features",
    "title": "Audio Features",
    "content": "## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Voice-based Features We will start with a discussion of useful speech features, and then proceed to some case studies where these features were used to determine contextual information. ### Voice Feature: Mel-frequency Spectral Coefficients (MFCC) As with accelerometer data, the core of a voice-based emotion sensing approach is obtaining good features. Perhaps the most important feature and widely used in voice is Mel-frequency Spectral Coefficients (MFCC). MFCC is such a useful voice feature that it deserves to be discussed on its own. The MFCC feature (and variants) has been heavily used in speech recognition, for example, it is part of how the Siri software on an iPhone recognizes your command. ##### What exactly is MFCC and how does it work? Sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of sound being produced, which in turn can help us design speech processing systems. The idea in MFCC is to extract features that closest to human perception of voice, since we care about humans interpret speech. The Mel scale relates perceived frequency, or pitch, of a pure tone to its actual measured frequency. Humans are much better at discerning small changes in pitch at low frequencies than they are at high frequencies. Incorporating this scale makes our features match more closely what humans hear. #### The MFCC computation pipeline The pipeline by which speech is converted to MFCC features is shown above. The figure looks daunting at first glance, but we will focus on providing you a high-level intuition of how to obtain MFCC features, and how these features work. **Step 1: Windowing.** Audio is continuously changing, so we break it down into short segments where we assume that things have been relatively constant. The tradeoff is that if our window is too short, we have too few samples to obtain a consistent result, and if it is too long, we have too many samples to obtain a good result. Typically, voice is broken down into 20-40ms segments. **Step 2: Power spectrum.** The next step is to calculate the power spectrum of each frame. This is motivated by the human cochlea (an organ in the ear) which vibrates at different spots depending on the frequency of the incoming sounds. Depending on the location in the cochlea that vibrates (which wobbles small hairs), different nerves fire informing the brain that certain frequencies are present. Our periodogram estimate performs a similar job for us, identifying which frequencies are present in the frame. **Step 3. Apply Mel Filterbank.** The periodogram spectral estimate still contains a lot of information that is unnecessary for speech recognition. In particular the cochlea can not discern the difference between two closely spaced frequencies. This effect becomes more pronounced as the frequencies increase. So, this step is just a way to combine the frequency spectrum into bins that is similar to how our ear perceives voice. The first filter is very narrow and gives an indication of how much energy exists near 0 Hertz where human hearing is very sensitive to variations. As the frequencies get higher our filters get wider as we become less concerned about variations. The formula for converting from frequency to Mel scale is: To go from Mels back to frequency: **Step 4. Logarithm of the Mel filterbank.** Once we have the filterbank energies, we take the logarithm of them. This is also motivated by human hearing: we don't hear loudness on a linear scale. Generally to double the percieved volume of a sound we need to put 8 times as much energy into it. This means that large variations in energy may not sound all that different if the sound is loud to begin with. This compression operation makes our features match more closely what humans actually hear. **Step 5. DCT of the log filterbank.** The final step is to compute the DCT of the log filterbank energies. This step is a bit harder to understand without a signal processing background. Intuitively, the idea is that there are a lot of correlations between the log filterbank energies, and this step tries to extract the most useful and independent features. At the end of the DCT, you get 13 MFCC coefficients; typically, these are combined with a few other features extracted through the same pipeline such as first derivative and second derivative coefficients. We won’t discuss them in detail here. Together, you get a 39 element acoustic vector that are the core features used in speech processing algorithms. ### Other Audio Features While MFCC are a very useful feature, this by no means the only one. In fact, there are many other features that are particularly useful in understanding emotional content of speech. Emotional speech processing tries to recognize the user's emotional state by analyzing speech patterns. You might wonder how features that capture emotion differ from those described above and used in speech processing. The distinction is that when we discussed MFCC and related features, we wanted to capture the _content of sound_, whereas with emotion processing, we want to capture the _sound characteristics a.k.a. prosody_. An intuitive distinction is that the content is about the words in speech, i.e., what we are saying, whereas the prosody is about the sound characteristics of speech, i.e., how we say it. In terms of acoustics, the prosodics of oral languages involve variation in syllable length, loudness, pitch, and the formant frequencies of speech sounds. We discuss some of these features below. * **Pitch:** Pitch describes how a listener perceive a sound. A sudden increase in pitch can often be perceived as high activation, such as anger, whereas low variance of pitch is often conceived as low energy, for example, sadness. * **Intensity:** The intensity reflects the effort to produce speech. Studies showed that angry utterance usually displays rapid rise of energy, and on the contrary sad speech usually is characterized by low intensity. Based on the observation, we need features that can describe the overall energy level and some momentary energy `onset' and `offset'. * **Temporal Aspects:** In temporal aspects are measures that can describe speech rate and voice activity (i.e., pauses). Some research showed that those two temporal properties may be affected by emotions. For example, sadness often result in slower speech and more pauses. * **Voice Quality:** Emotions may also influence the voice quality of utterances. For example, some voice becomes sharp or jagged while some voice sounds soft. Glottal waveforms are useful to describe these sound characteristics. As illustrated in the above figure, a glottal (flow) waveform represents the time that the glottis is open (with air flowing between vocal folds), and the time the glottis is closed for each vibrational cycle. In addition, an open phase can be further broken down into opening and closing phases. If there is a sudden change in air flow (i.e., shorter open and close phases), it would produce more high frequency and the voice therefore sounds more jagged, other than soft. To capture it, we need features that describe timings of the phases and the ratios of closing to opening phase, open phase to total cycle, closed phase to total cycle, opening to open phase, and closing to open phase. * **Spectrogram:** A spectrogram can describe the energy distribution across frequency bands (as shown above). The reason was that the emphasis on certain frequency may be speaker dependent and may be used to reflect emotions. * **Other Statistical Measures:** There are several basic statistical measures, which can help represent all possible dynamics which might be affected by emotions. ### References [1] Speech Emotion Classiﬁcation using Machine Learning Algorithms, S. Casale, A. Russo, G. Scebba [3] OpenEAR - Introducing the Munich Open-Source Emotion and Affect Recognition Toolkit, Florian Eyben, Martin Wollmer, and Bj ¨ orn Schuller [4] StressSense: Detecting stress in unconstrained acoustic environments using smartphones, Lu, H., Frauendorfer, D., Rabbi, M., Mast, M. S., Chittaranjan, G. T., Campbell, A. T., ... & Choudhury, T. ",
    "url": "/mhealth-course/chapter5-voiceanalytics/ch5-audiofeatures.html",
    
    "relUrl": "/chapter5-voiceanalytics/ch5-audiofeatures.html"
  },"46": {
    "doc": "Data Smoothing",
    "title": "Data Smoothing",
    "content": "## Data Smoothing and Filtering {: .no_toc } ## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Chapter 1: Sensor data smoothing and filtering Most sensor data is affected to some extent by **noise**, that is unexplained variations in the data that in many cases is uninterpretable, and in almost all cases is not of interest to us. Data analysis is often considerably simpler if this noise can be removed from the data. This chapter provides an overview of the sources of noise in typical sensor data and the methods by which noise can be removed. ### Part 1: Introduction to Smoothing and Filtering [[notes](ch1-intro.html)] [[slides](https://drive.google.com/file/d/1jo4hHP76vGtyNnOeWMaLCT0mmdynV-Nd/view?usp=drive_link)] We start by explaining the difference between information and noise, and describe what causes noise in sensor data. We then show different examples of noisy signals and what are some common sources of noise in these signals. ### Part 2: Time-series Smoothing and Filtering [[notes](ch1-timedomainfiltering.html)] [[slides](https://drive.google.com/file/d/1jo4hHP76vGtyNnOeWMaLCT0mmdynV-Nd/view?usp=drive_link)] Noise removal techniques can be divided into two class. The first is time-domain approaches, where the information is contained in the amplitude of a signal. For example, you may be interested in the temperature of this room, the orientation of your phone, your location and driving trajectory, and so on. All of these contain information in the time domain. ### Part 3: Frequency-domain Filtering [[notes](ch1-freqdomainfiltering.html)] [[slides](https://drive.google.com/file/d/1jo4hHP76vGtyNnOeWMaLCT0mmdynV-Nd/view?usp=drive_link)] The second is frequency-domain approaches, which removes noise that is periodic in nature. For example, accelerometer readings when you walk show periodic motion and your heart beats in a quasi-periodic manner. Many noise sources are similarly periodic and can be removed using frequency domain filtering methods. ### Notebook 1: Time Domain Noise Removal [[html](/mhealth-course/chapter1-noise/notebooks/Chapter1-TimeDomainNoiseRemoval.html)] [[ipynb](/mhealth-course/chapter1-noise/notebooks/Chapter1-TimeDomainNoiseRemoval.ipynb)] This notebook shows a few examples of time-series signals and how different time-domain smoothing methods (moving average, exponentially weighted moving average, and median filtering) work on this data. ### Notebook 2: Frequency Domain Noise Removal [[html](/mhealth-course/chapter1-noise/notebooks/Chapter1-FreqDomainNoiseRemoval.html)] [[ipynb](/mhealth-course/chapter1-noise/notebooks/Chapter1-FreqDomainNoiseRemoval.ipynb)] This notebook shows a few examples of time-series signals and how different frequency-domain noise removal methods (low-pass, high-pass, and notch filter) work on this data. ### Notebook 3: Fourier Decomposition [[html](Chapter1-Fourier-Denoising.html)] [[ipynb](Chapter1-Fourier-Denoising.ipynb)] The Fourier transform is a tool that allows you to take a signal and see the power of each frequency in it. This example notebook shows how you can remove frequency-domain noise from a signal using a fourier transform. ### Notebook 4: Butterworth Filtering [[html](Chapter1-ButterworthFilter.html)] [[ipynb](Chapter1-ButterworthFilter.ipynb)] Frequency-domain noise can be removed by a combination of frequency filters. A low-pass filter can remove high frequency components while letting through low frequency components. A high pass filter does the reverse and lets high frequency components through while removing low frequency components. A notch filter removes a specific frequency from the signal. In this notebook, we show how to use a filter called a 'butterworth filter' to remove noise. A Butterworth filter is a popular frequency domain 'lowpass' filter that can remove high frequency noise while only letting the low frequencies through. Since many signals we deal with such as steps, heartbeats and breathing are low frequency signals i.e. only a few repetitions per minute, this is a good approach. ",
    "url": "/mhealth-course/chapter1-noise/chapter1.html",
    
    "relUrl": "/chapter1-noise/chapter1.html"
  },"47": {
    "doc": "Step Counting",
    "title": "Step Counting",
    "content": "## Designing a Pedometer and Calorie Counter {: .no_toc } ## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Chapter 2: Designing a Pedometer and Calorie Counter Pedometers or step counters are now popular as an everyday exercise progress monitor and motivator. The increasing popularity of these devices can be attributed to several reasons. First, many people are known to overestimate their level of activity, hence these devices can provide more reliable feedback to an individual about how much or little they move during the day. Second, they provide instant and constant feedback about activity levels, making it possible to “gamify” by providing credits for every step an individual takes. Third, they can encourage individuals to compete with themselves in getting fit and losing weight. ### Part 1: Introduction to Step Counting [[notes](ch2-intro.html)] [[slides](https://drive.google.com/file/d/1joGfgzsmzz55cQmhGwQvQImarvfrmdbF/view?usp=drive_link)] We start by explaining what an accelerometer on a phone or fitness band measures when you walk, and why it is convenient to work with the magnitude signal. ### Part 2: Sources of noise [[slides](https://drive.google.com/file/d/1joGfgzsmzz55cQmhGwQvQImarvfrmdbF/view?usp=drive_link)] We describe some of the real-world challenges in accurately estimating the number of steps including common sources of noise such as the device moving around, differences across people in terms of their gait patterns, and others. ### Part 3: Designing a Step Counter [[notes](ch2-stepcounter.html)] [[slides](https://drive.google.com/file/d/1joGfgzsmzz55cQmhGwQvQImarvfrmdbF/view?usp=drive_link)] This part describes how you design a step counter and how you detect the number of steps ### Part 4: Calorie Tracking [[notes](ch2-calories.html)] [[slides](https://drive.google.com/file/d/1joGfgzsmzz55cQmhGwQvQImarvfrmdbF/view?usp=drive_link)] Step counters not only tell us the number of steps but also a (rough) estimate of the number of calories burned. We now look at how we can map from steps to calories. ### Notebook: Step Counting with Find Peaks [[html](/mhealth-course/chapter2-steps/notebooks/Chapter2-StepCounting.html)] [[ipynb](notebooks/Chapter2-StepCounting.zip)] This notebook shows a step counter using `find_peaks` and applies it to a number of sample sensor logs. The different logs correspond to different sensor placements (left pocket, right pocket, wrist), and to different walking patterns (e.g. with delays between short burst of steps). The notebook shows how tweaking the `prominence` and `width` parameters can allow you to fine-tune the performance of the step counter. ",
    "url": "/mhealth-course/chapter2-steps/chapter2.html",
    
    "relUrl": "/chapter2-steps/chapter2.html"
  },"48": {
    "doc": "Activity Recognition",
    "title": "Human Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/chapter3.html#human-activity-recognition",
    
    "relUrl": "/chapter3-activityrecognition/chapter3.html#human-activity-recognition"
  },"49": {
    "doc": "Activity Recognition",
    "title": "Table of Contents",
    "content": ". | Chapter 3: Activity Recognition using Inertial Sensors [Slides] . | Part 1: Activity classification vs Step counting [notes] | Part 2: Visualizing accelerometer signals for common activities [notes] | Part 3: Time-domain features for classification [notes] | Part 4: Frequency-domain features for classification [notes] | Part 5: Decision tree classification [notes] | Part 5: Evaluating Classifier Performance [notes] | . | . ",
    "url": "/mhealth-course/chapter3-activityrecognition/chapter3.html#table-of-contents",
    
    "relUrl": "/chapter3-activityrecognition/chapter3.html#table-of-contents"
  },"50": {
    "doc": "Activity Recognition",
    "title": "Chapter 3: Activity Recognition using Inertial Sensors [Slides]",
    "content": "Having discussed the design of a pedometer that uses an accelerometer, let us now turn to the problem of activity recognition, a task which involves identifying the physical activity a user is performing. Activity recognition is useful because it permits us to gain useful knowledge about the habits of millions of users passively—just by having them carry cell phones in their pockets. Such techniques can have a wide range of applications, including automatic customization of the mobile device’s behavior based upon a user’s activity (e.g., sending calls directly to voicemail if a user is jogging) and generating a daily/weekly activity profile to determine if a user is performing a healthy amount of exercise. Part 1: Activity classification vs Step counting [notes] . We describe the difference between detection and classification, and provide an overview of a classification pipeline. We start with how to collect labeled data, and visualize the signal for a variety of common activities like walking, jogging, walking upstairs, walking downstairs, sitting and standing. Part 2: Visualizing accelerometer signals for common activities [notes] . We now visualize the accelerometer signal for a variety of common activities like walking, jogging, walking upstairs, walking downstairs, sitting and standing. This will help us get an intuitive feel for how the accelerometer signal differs for different activities. Part 3: Time-domain features for classification [notes] . One of the crucial steps in classifying sensor data is identifying distinguishing features in the data. Some of these features are time-domain statistics of the data such as average and standard deviations. We describe some commonly used features. Part 4: Frequency-domain features for classification [notes] . Another important class of features is frequency domain features. We describe some basic frequency domain features. Part 5: Decision tree classification [notes] . We start by explaining how to perform activity classification using a decision tree. Part 5: Evaluating Classifier Performance [notes] . We look at how to evaluate classifier performance. ",
    "url": "/mhealth-course/chapter3-activityrecognition/chapter3.html#chapter-3-activity-recognition-using-inertial-sensors-slides",
    
    "relUrl": "/chapter3-activityrecognition/chapter3.html#chapter-3-activity-recognition-using-inertial-sensors-slides"
  },"51": {
    "doc": "Activity Recognition",
    "title": "Activity Recognition",
    "content": " ",
    "url": "/mhealth-course/chapter3-activityrecognition/chapter3.html",
    
    "relUrl": "/chapter3-activityrecognition/chapter3.html"
  },"52": {
    "doc": "Heart Rhythm Sensing",
    "title": "Heart Rhythm Sensing",
    "content": "## Sensing Heart Rhythm {: .no_toc } ## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Chapter 2: Measuring the activity of the human heart Cardiovascular disease — pathologies of the heart, blood vessels and the vascular system of the brain — claims more lives than anything else, accounting for nearly one-third of deaths worldwide. Those deaths are projected to increase until at least 2030. Thanks to advances in wearable health-tracking devices, people can monitor their heart rhythms much easier now. Wrist-worn smartwatches and fitness monitors or wearables have been widely adopted by consumers and are currently gaining increased attention by researchers for their potential contribution to digital measurement of heart rhythm, thereby providing early detection of irregularities as well as to provide a more holistic view of daily cardiovascular health. #### Part 1: How can we measure activity of the human heart [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] Interestingly, there are many different ways of monitoring the human heart, ranging from wristworn fitness bands that largely use optical heart rate monitors to conventional Electrocardiograms and Stethoscopes, to more recent development of contactless monitoring devices that use Ballistocardiography or Mechanocardiography. #### Part 2: Photoplethysmography (PPG) [[notes](ch4-ppg.html)] [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] The most common method for monitoring heart rhythm with a wearable device is by using an optical heart rate sensor that uses Photoplethysmography (PPG) i.e. collecting pulse rate or volumetric changes in blood flow that act as a surrogate for heart rate (HR). #### Part 3: PPG Noise sources [[notes](ch4-ppg.html)] [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] PPG is challenging to analyze in real-world environments since the optical signal can be noisy (due to device movement, environment lighting), and also the signal quality can vary across individuals (due to skin tone, underlying health conditions that lead to lower than normal blood flow). #### Part 4: Estimating Heart rate and Breathing rate from PPG [[notes](ch4-ecg-ppg-analysis.html)] [[slides 36-40](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] We look at how one can extract heart rate and breathing rate from PPG using techniques very similar to those that you used for counting steps. After removing noise and baseline wander from a PPG signal, count the peaks to get a rough estimate of heart rate. Then look for the variation in the peak height to obtain information about breathing rate. #### Part 5: Estimating Oxygen Saturation from PPG [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] The PPG signal can also be used to measure blood oxygenation; this functionality is used in the Apple Watch 6 but is also the principle behind Pulse Oximeters. Oxygen saturation measurement has become particularly important during pandemic since it is one of the early signals that an individual may be infected. any people with COVID-19 have low levels of oxygen in their blood, even when they feel well. Low oxygen levels can also be an early warning sign that medical care is needed. #### Part 6: Phonocardiography, Ballistocardiography and Mechanocardiography [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFLWpBMnV5cHNCYzA/view?usp=sharing&resourcekey=0-wID7JSxr1I4jmcmoAzvfgw)] To conclude this lesson, we provide a brief overview of other ways of measuring heart rhythm. While ECG uses electrodes and PPG uses optical sensors, there are other ways of measuring the human heart. Bedside sleep monitors can use radar signals to measure the ballistic signals of the heart and calculate heart rate; mattresses with sensitive pressure sensors can do the same. #### Notebook 1: Analyzing Noisy PPG (Assignment) #### Notebook 2: Extracting Heart Rate from Noisy PPG [[html](/mhealth-course/chapter4-heartrhythm/Chapter4-HeartRate-from-Noisy-PPG.html)] [[ipynb](/mhealth-course/chapter4-heartrhythm/Chapter4-HeartRate-from-Noisy-PPG.ipynb)] In this notebook we'll go over how to analyze a noisy PPG signal and extract heart rate. We will take several examples, a very clean PPG signal, a noisy PPG signal with motion artifacts, a noisy PPG signal from a smartwatch, and a signal from a smartring. ",
    "url": "/mhealth-course/chapter4-heartrhythm/chapter4.html",
    
    "relUrl": "/chapter4-heartrhythm/chapter4.html"
  },"53": {
    "doc": "Vocal Biomarkers",
    "title": "Vocal Biomarkers",
    "content": "## Voice-based Health Analytics {: .no_toc } ## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Chapter 5: Voice-based Health Analytics Voice is an extremely useful and important part of health analytics, and an amazing amount of information can be gleaned from recorded audio from a simple microphone. As microphones become more powerful and more ubiquitous, the possibility that computing systems will be able to monitor vocal biomarkers and extract changes in health patterns has exciting possibilities for understanding ourselves and in anticipating problems. #### Part 1: What are some of the useful features we can extract from voice? [[notes](ch5-audiofeatures.html)] [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFRm13c3NqREdxTUU/view?usp=sharing&resourcekey=0-m5VIqT4c3JBhd4kykBkIPA)] We will start with a discussion of useful speech features. In particular, we focus on MFCC features which are closest to human perception of voice including how we perceive frequency and volume. We also discuss other features that can be combined with MFCC to understand emotional content of speech. #### Part 2: What health states can be classified with audio biomarkers? [[notes](ch5-audioclassification.html)] [[slides](https://drive.google.com/file/d/0Bw0KEeNzOgzFRm13c3NqREdxTUU/view?usp=sharing&resourcekey=0-m5VIqT4c3JBhd4kykBkIPA)] We then look at some interesting ways in which voice processing can be used for obtaining measures of health. There are many ways in which audio biomarkers may be used -- for example, should a person undergo further screening for depression or other mental health issues? are the changes in vocal patterns indicative of worsening neurological disease? what is the mood of an individual? is a person stressed? and so on. In this chapter, we will look at some interesting ways in which voice processing can be used for obtaining measures of health. #### Notebook 1: Determining Gender from Speech [[html](/mhealth-course/chapter5-voiceanalytics/Chapter5-GenderClassification-Voice.html)] [[ipynb](/mhealth-course/chapter5-voiceanalytics/Chapter5-GenderClassification-Voice.ipynb)] This notebook tries to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by analyzing frequency range of 0hz-280hz (human vocal range). ",
    "url": "/mhealth-course/chapter5-voiceanalytics/chapter5.html",
    
    "relUrl": "/chapter5-voiceanalytics/chapter5.html"
  },"54": {
    "doc": "Measuring Sleep Patterns",
    "title": "Measuring Sleep Patterns",
    "content": "## Measuring Sleep Patterns {: .no_toc } ## Table of Contents {: .no_toc .text-delta } 1. TOC {:toc} --- ## Chapter 6: Measuring Sleep Patterns Sleep is increasingly recognized as important to public health, with sleep insufficiency linked to motor vehicle crashes, industrial disasters, and medical and other occupational errors. Unintentionally falling asleep, nodding off while driving, and having difficulty performing daily tasks because of sleepiness all may contribute to these hazardous outcomes. Persons experiencing sleep insufficiency are also more likely to suffer from chronic diseases such as hypertension, diabetes, depression, and obesity, as well as from cancer, increased mortality, and reduced quality of life and productivity. Sleep insufficiency may be caused by broad scale societal factors such as round-the-clock access to technology and work schedules, but sleep disorders such as insomnia or obstructive sleep apnea also play an important role. An estimated 50-70 million US adults have sleep or wakefulness disorder. Notably, snoring is a major indicator of obstructive sleep apnea. Two new reports on the prevalence of unhealthy sleep behaviors and self-reported sleep-related difficulties among U.S. adults provide further evidence that insufficient sleep is an important public health concern. ### Sleep-Related Unhealthy Behaviors One of the major surveys conducted by the Center for Diseases and Control is on Behavioral Risk Factor Surveillance. In 2008, this questionnaire included a question on perceived insufficient rest or sleep. The analysis [2] determined that, among 74,571 adult respondents in 12 states, 35.3\\% reported &lt;7 hours of sleep during a typical 24-hour period, 48.0\\% reported snoring, 37.9\\% reported unintentionally falling asleep during the day at least once in the preceding month, and 4.7\\% reported nodding off or falling asleep while driving at least once in the preceding month. The National Department of Transportation estimates drowsy driving to be responsible for 1,550 fatalities and 40,000 nonfatal injuries annually in the United States. Sleep also affects ability to perform daily activities, as shown in the figure below. The two bars show the fraction of people who responded to the survey who had difficulty performing the specific activity. The dark blue bar corresponds to individuals who had difficulty while sleeping less than 7 hours a day, and the light blue bar corresponds to those who had difficulty in performing the tasks and slept 7-9 hours day. The conclusion seems rather clear - in general, less sleep seems correlated with greater difficulty in concentration, memory, driving, workplace performance and other daily activities. ![alt_text](images/sleep-behavior.png \"image_tooltip\") ### How Much Sleep Do We Need? And How Much Sleep Are We Getting? How much sleep we need varies between individuals but generally changes as we age. The National Sleep Foundation suggests that school-age children (5-10 years) need 10-11 hours of sleep daily, teens (10-17 years) need 8.5-9.5 hours, and adults need 7-9 hours. According to data from the National Health Interview Survey, nearly 30\\% of adults reported an average of ≤6 hours of sleep per day in 2005-2007. In 2009, only 31\\% of high school students reported getting at least 8 hours of sleep on an average school night. ![alt_text](images/sleep-difficulty.png \"image_tooltip\") ## How to measure sleep using wearable sensors? The key question in our discussion is how to measure sleep. The traditional approach to sleep research is called polysomnography, an intensive high-fidelity approach that typically requires more than 20 wires to be hooked up to the test subject. This noodle soup of nodes and cables measure everything from brain activity to eye movement, leg movement, breathing rate and heart rate. This is the stuff of sleep labs, and though the measurements are highly detailed and thorough, they come with a catch: it is very cumbersome, and by requiring somebody to go to a sleep lab and get hooked up to be measured, you're inherently messing up the experiment because the conditions have changed. Here's why: Collecting the data requires the person with sleep troubles to leave their home environment — their own bedroom, their own bed, their own sheets. That's not restful. Plus with all those wires and nodes, the sleep subject is bound to be disturbed by being literally tied down. So while the measurements may be precise and exacting, the experiment may not be replicating the same kind of sleep. If you have ever participated in a sleep study, you would know that EEG (brain waves) is the most common way of measuring sleep. Of the modalities used in sleep studies, the most important one is EEG, and a wealth of information is available about how different EEG patterns can be used to distinguish between the sleep stages, and understand the quality of sleep. The figure below shows EEG signals for different sleep stages that a typical individual goes through during sleep. ![alt_text](images/eeg-sleep.jpg \"image_tooltip\") Before we move on, let us first briefly discuss the sleep stages and their importance. It is widely thought that there are four stages of sleep as an individual progressively goes from light to deep sleep. These sleep stages are interspersed with REM sleep, where the brain is alert and one has vivid dreams. While researchers are not entirely sure of the role of all the sleep stages, it is considered that the sleep stages aid physical and mental recovery, and memory consolidation. REM sleep is considered to aid in creative thinking and making new connections between neurons. While EEG gives accurate data about sleep stages, as you might imagine EEG is cumbersome for daily sleep sensing, hence we need a more unobtrusive sleep sensing modality. Lets look at some of the other sensor modalities that are used in polysomnography. The figure below shows that different sleep stages also induce changes in other physiological parameters like the breathing pattern and ECG waveform. ![alt_text](images/sleep-sensors.jpg \"image_tooltip\") ### Using respiration for sensing sleep How about using a respiration sensor for measuring sleep? Intuition suggests that your breathing must become slower and more controlled when you sleep, and this is indeed what datasets on sleeping suggest. The figure above shows the breathing pattern when you are in sleep state vs waking state, and shows that you indeed have substantial differences between the REM and non-REL states when it comes to breathing patterns. Let us delve into this a bit more. If you look at the figure below, you can see that NREM sleep is much more uniform, whereas there is more irregularity, and longer inhalations during REM sleep, perhaps as a result of the dreams! ![alt_text](images/respiration-sleep.png \"image_tooltip\") Similarly, ECG changes can be observed during different sleep stages, particularly in REM vs Non-REM sleep. However, wearing a respiration or ECG sensor on the chest can be quite cumbersome, making this a somewhat intrusive way of measuring sleep stage. ### Using activity for sensing sleep Can we measure sleep parameters using our trusty accelerometer? One of the most interesting, and non-obvious, uses of a simple accelerometer is for measuring sleep quality. There are three broad categories of consumer sleep tracking devices that use accelerometers. The first is smartphone based “sleep apps” in the iPhone or Android App Stores - you place the device on the mattress, and the accelerometer on the smartphone is used to do the rest. The second is a wristworn accelerometer, and many fitness bands (fitbit, microsoft band, jawbone up etc) use wrist-based sensors to sense sleep patterns. The third is sleep trackers that require you to place an accelerometer-instrumented measurement pad beneath your mattress, and these can sense tiny movements of the mattress and relay it to a bedside device. This is somewhat similar to the smartphone app-based approach, except that they can use a larger sensing surface underneath the mattress rather than rely on a single point sensor on a smartphone. Why do we need sleep trackers? Besides just getting information regarding sleep quality, several sleep trackers also provide information regarding lighting levels and environmental noise levels so that you can identify why your sleep quality may have been affected. Another interesting application is as a sleep stage-aware alarm clock. For example, here’s what the “Sleep Cycle” app on iTunes has to say: “Have you ever woken up feeling completely wrecked when the alarm clock goes off, despite the fact that you have slept “enough” hours? When this happens you have probably been awakened during a deep sleep phase, and your whole day can turn into one long zombie marathon. Other days you spring out of bed with a smile on your face, feeling completely rested even though you shouldn’t. As the alarm clock goes off, chance seems to play a big role in how your day will become. But does it really have to be that way? This is where the Sleep Cycle alarm clock application comes into play. During the night you go from light sleep to deep sleep, occasionally entering into a dream state which is called REM-sleep. These are things that your normal alarm clock does not care about, and will go off at the set time regardless of whether you are in a light sleep phase or in the deepest sleep. However, since you move differently in bed during the different phases, the Sleep Cycle alarm clock is able to use the accelerometer in your iPhone to monitor your movement and determine which sleep phase you are in. Sleep Cycle then uses a 30 minute alarm window that ends at your set alarm time and wakes you in your lightest sleep phase.” So, how do you detect sleep using an accelerometer. Let us first look at classifying when you are asleep vs awake. This should be relatively simple - we tend to move a lot more when we are awake then when we are asleep. The figure below shows the overall accelerometer activity (energy) during a vigilant phase vs while sleeping. Clearly, there are differences that would help classification. ![alt_text](images/eda.png \"image_tooltip\") The figure below shows an example where motion-based detection of the wake state is shown on the top panel, and the ground truth of sleep states is shown below. What type of features would you use for this classification task? Intuitively, you can try to segment data into short time windows (30-60 seconds), and extract the peak acceleration, average acceleration, number of movements, etc for this window, and then feed these features to a classifier that tries to decide whether you are awake or asleep. However, one would have to be careful of confounding states - for example, if you are in front of your computer typing away, can this get detected as being asleep? Or if you are watching television on a couch, can this be classified as being asleep? and so on. ![alt_text](images/motion-sleepstages.png \"image_tooltip\") If you are using a smart wristband for sleep detection, one approach to deal with confounders would be to augment the accelerometer energy with also postural information as shown in the figure below. If your wristband has a compass, you can use this sensor to get absolute orientation, but even if it only has an accelerometer, you can narrow down the orientation by just looking at the direction of the gravity vector. Before you sense orientation, however, you will need to calibrate your algorithm since you may not know a priori what direction is face-up vs face-down, but this may be something that only needs to be done once for a device. ![alt_text](images/posture.png \"image_tooltip\") The next question is whether we can classify sleep stage. This is difficult to do in a reliable manner using just an accelerometer, but people have tried! Some of you might have noticed that you tend to have some jerky motions when you are about to slip into a deep sleep mode. The general idea is to take sense this pattern and use them to detect your current sleep stage. However, this is a difficult problem that research or commercial systems haven’t solved yet. (Many claim to identify sleep stage, but results tend to be quite unreliable). ### Using electrodermal activity to sense sleep Electrodermal activity (EDA) is widely used in psychophysiology and provides a measure of sympathetic nervous system (SNS) activity, where the SNS is one of the main branches of the autonomic nervous system. Some recent wrist-worn devices including the Microsoft Band and Empatica wristbands come equipped with Galvanic Skin Response sensors which measures EDA activity, hence this is another interesting modality that we can use in sleep sensing. Studies on EDA during sleep have shown that EDA is more likely to appear elevated with high frequency “storm” patterns during deep sleep, where an EDA “storm” region (Burch [5]) refers to a region of EDA with a burst of high frequency peaks. Burch originally quantified a storm as a minimum of five galvanic skin responses (GSRs)/min for at least ten consecutive minutes of sleep, but other studies have used slightly different thresholds. Studies on the use of EDA have indicated that EDA storms can distinguish wake and sleep and indicate sleep onset, and can also be used to separate REM sleep from NREM sleep. Features used in this classification included the number of peaks in storms, durations of storms, peak frequency, amplitude and onset time of the first storm. While promising, this is an area where there is not solid evidence, so much more remains to be done before it is in widespread use. ![alt_text](images/eda.png \"image_tooltip\") ### Using radar and ultrasound doppler to sense sleep All the methods that we have discussed above involve wearable devices, but there have also been studies that have attempted to use passive methods to sense sleep. One technique that has seen recent interest is the use of doppler effect to sense chest movements and thereby detect the respiration and heart-rate patterns, from which information can be gleaned about sleep [9]. The underlying idea in this approach is to track fine-grained movements of the chest of an individual who is sleeping by using a doppler ranging radar. The chestwall makes two movements of interest - tiny movements are the result of ballistic movements of the chest every time the heart pumps (ballistocardiography or BCG) and larger movements corresponding to respiration cycles. Both of these can (in principle) be detected using a doppler radar. While interesting, this is certainly early work and needs more work to validate in real-world settings. ### References 1. CDC: Insufficient sleep is a public health problem. National Health and Nutrition Examination Survey (NHANES) - Sleep Disorders Questionnaire Analysis. [Link](http://www.cdc.gov/mmwr/PDF/wk/mm6008.pdf) 2. Institute of Medicine. Sleep Disorders and Sleep Deprivation: An Unmet Public Health Problem. Washington, DC: The National Academies Press; 2006. 3. US Department of Transportation, National Highway Traffic Safety Administration, National Center on Sleep Disorders Research, National Heart Lung and Blood Institute. Drowsy driving and automobile crashes. 4. Schoenborn CA, Adams PF. Health behaviors of adults: United States, 2005–2007. National Center for Health Statistics. Vital Health Stat 10(245). 2010. 5. CDC. Youth Risk Behavior Surveillance—United States, 2009. MMWR 2010;59:SS-5. 6. Natural Patterns of Sleep, Division of Sleep Medicine at Harvard Business School. [Link](http://healthysleep.med.harvard.edu/healthy/science/what/sleep-patterns-rem-nrem) 7. How to monitor sleep stages. Bioshare. [Link](http://www.bioshare.info/en/sleep) 8. What are REM and non-REM sleep. WebMD. [Link](http://www.webmd.com/sleep-disorders/guide/sleep-101) 9. [Got Sleep Problems? Try tracking your sleep with a radar.](http://www.technologyreview.com/news/539961/got-sleep-problems-try-tracking-your-rest-with-radar/) MIT Technology Review, 2015 10. Statistical Characterization of Actigraphy Data for Sleep/Wake Assessment, International Journal of Bioelectromagnetism - Vol. 15 11. [The Role of Actigraphy in the Study of Sleep and Circadian Rhythms](http://www.aasmnet.org/Resources/PracticeReviews/cpr_Actigraphy.pdf), American Academy of Sleep Medicine Review Paper ",
    "url": "/mhealth-course/chapter6-sleepsensing/chapter6.html",
    
    "relUrl": "/chapter6-sleepsensing/chapter6.html"
  },"55": {
    "doc": "Mobile Sensing &amp; Analytics",
    "title": "Mobile Health Sensing and Analytics",
    "content": " ",
    "url": "/mhealth-course/#mobile-health-sensing-and-analytics",
    
    "relUrl": "/#mobile-health-sensing-and-analytics"
  },"56": {
    "doc": "Mobile Sensing &amp; Analytics",
    "title": "Table of Contents",
    "content": ". | CS328: Mobile Health Sensing and Analytics . | Chapter 1: Introduction to Smoothing and Filtering [Slides] | Chapter 2: How to Design a Step Counter [Slides] | Chapter 3: Photoplethysmography [Slides] | Chapter 4: Activity Recognition using Inertial Sensors [Slides] | Chapter 5: Voice-based Health Analytics [Slides] | Chapter 6: Sleep Sensing [Slides] | . | . ",
    "url": "/mhealth-course/#table-of-contents",
    
    "relUrl": "/#table-of-contents"
  },"57": {
    "doc": "Mobile Sensing &amp; Analytics",
    "title": "CS328: Mobile Health Sensing and Analytics",
    "content": "Fitness gadgets such as ​Fitbit​, ​Apple Watch,​ ​Android Wear,​ and smartphone apps such as ​RunKeeper​ and M​oves​ calculate activity patterns, calories burned each day, track sleep patterns, and compute heart rate. We will learn how to build the computational elements for developing such applications by leveraging various sensors on smartphones, including the accelerometer, camera, microphone, and GPS. This is a hands-on course where students learn by doing! . Each of the following lessons has associated slides, videos, course notes, and Python notebooks that explain and illustrate the main concepts. Chapter 1: Introduction to Smoothing and Filtering [Slides] . This chapter provides an introduction into the first steps in processing sensor data i.e. removing noise. The chapter describes sources of noise that present a challenge for analyzing sensor signals and describes methods to remove noise including time-domain noise removal and frequency-domain noise removal techniques. This chapter gives you the basic tools for removing noise and will help you with your first assignment which involves noise removal from accelerometer and ECG data. Chapter 2: How to Design a Step Counter [Slides] . The second chapter provides an overview of how step counters (also referred to as pedometers) work. We look at what characteristics of steps are sufficiently distinctive to be measured accurately by a step counter. We also look at how placement of the phone or smartwatch can affect the signal, as well as other sources of error. We discuss methods to deal with these errors. This chapter will give you ideas on how to solve the assignment that asks you to design a step counter and check how it works for different sensor placements. Chapter 3: Photoplethysmography [Slides] . The third chapter describes how fitness bands/smartwatches like Fitbit and Apple Watch measure heart rate. We look at how optical heart rate monitoring sensors work, and how to extract important metrics like heart rate and breathing rate from the photoplethsmography signal. We also look at the challenges involved in estimating heart rhythm accurately at the population scale. This chapter is useful for your assignment on estimating heart rate from the PPG signal obtained when you place your finger on a smartphone camera. Chapter 4: Activity Recognition using Inertial Sensors [Slides] . The fourth chapter describes how to design a classifer for activity recognition i.e. the task which involves identifying the physical activity a user is performing. Activity recognition is useful because it permits us to gain useful knowledge about the habits of millions of users passively—just by having them carry cell phones or wear smart watches. We describe how you can build a simple classifier to recognize if a person is sitting, standing, jogging, walking upstairs, walking downstairs or driving a car. We also look at how to collect a training dataset and measure the performance of a classifier. Chapter 5: Voice-based Health Analytics [Slides] . Voice is an extremely useful and important part of health analytics. There are many interesting questions that can be answered with audio processing – for example, do changes in vocal pattern indicate symptoms of an underly neurological disease? how much social interaction does a person engage in? what is the emotional content of the voice? and so on. These indicators can be leveraged for understanding depression, stress, mood, and many other factors that are relevant to personal health. In this lesson, we discuss useful voice-based features that can be used for voice-based health analytics. Chapter 6: Sleep Sensing [Slides] . Sleep is increasingly recognized as important to public health, with sleep insufficiency linked to motor vehicle crashes, industrial disasters, and medical and other occupational errors. We discuss the importance of measuring sleep stages, and both traditional ways to measure sleep using polysomnography as well as how modern wearable devices can measure sleep and sleep stages. ",
    "url": "/mhealth-course/#cs328-mobile-health-sensing-and-analytics",
    
    "relUrl": "/#cs328-mobile-health-sensing-and-analytics"
  },"58": {
    "doc": "Mobile Sensing &amp; Analytics",
    "title": "Mobile Sensing &amp; Analytics",
    "content": " ",
    "url": "/mhealth-course/",
    
    "relUrl": "/"
  }
}
